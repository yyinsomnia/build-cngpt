{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyinsomnia/build-cngpt/blob/main/gpt_mao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 环境和依赖\n",
        "\n"
      ],
      "metadata": {
        "id": "3BIWez3s2klu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_fPxK2iaPZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15589308-321f-4e82-cc51-fae54bc91ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-20 16:06:55--  https://raw.githubusercontent.com/yyinsomnia/openai-ex/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42761 (42K) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]  41.76K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-07-20 16:06:55 (43.3 MB/s) - ‘input.txt’ saved [42761/42761]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/yyinsomnia/openai-ex/main/input.txt\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 传统的语言模型bigram"
      ],
      "metadata": {
        "id": "hbR7rffa2zCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9iboUIQbZll"
      },
      "outputs": [],
      "source": [
        "with open('input.txt') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4csMaMk4bkxQ",
        "outputId": "11a97b6a-bd69-4cae-9459-23f8a2d08bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the mao-poetic in characters is : 16498\n"
          ]
        }
      ],
      "source": [
        "print('length of the mao-poetic in characters is :', len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtOUUVQTdEyM",
        "outputId": "b0cdb3e0-afa7-4a8e-8411-de4aca36a112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 西江月·秋收起义\n",
            "\n",
            "军叫工农革命，\n",
            "\n",
            "旗号镰刀斧头。\n",
            "\n",
            "匡庐一带不停留，\n",
            "\n",
            "要向潇湘直进。\n",
            "\n",
            "地主重重压迫，\n",
            "\n",
            "农民个个同仇。\n",
            "\n",
            "秋收时节暮云愁，\n",
            "\n",
            "霹雳一声暴动。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 菩萨蛮·黄鹤楼\n",
            "\n",
            "茫茫九派流中国，\n",
            "\n",
            "沉沉一线穿南北。\n",
            "\n",
            "烟雨莽苍苍，\n",
            "\n",
            "龟蛇锁大江。\n",
            "\n",
            "黄鹤知何去？\n",
            "\n",
            "剩有游人处。\n",
            "\n",
            "把酒酹滔滔，\n",
            "\n",
            "心潮逐浪高！\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·和柳亚子先生\n",
            "\n",
            "饮茶粤海未能忘，索句渝州叶正黄。\n",
            "\n",
            "三十一年还旧国，落花时节读华章。\n",
            "\n",
            "牢骚太盛防肠断，风物长宜放眼量。\n",
            "\n",
            "莫道昆明池水浅，观鱼胜过富春江。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·冬云\n",
            "\n",
            "雪压冬云白絮飞，万花纷谢一时稀。\n",
            "\n",
            "高天滚滚寒流急，大地微微暖气吹。\n",
            "\n",
            "独有英雄驱虎豹，更无豪杰怕熊罴。\n",
            "\n",
            "梅花欢喜漫天雪，冻死苍蝇未足奇。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·到韶山\n",
            "\n",
            "别梦依稀咒逝川，故园三十二年前。\n",
            "\n",
            "红旗卷起农奴戟，黑手高悬霸主鞭。\n",
            "\n",
            "为有牺牲多壮志，敢教日月换新天。\n",
            "\n",
            "喜看稻菽千重浪，遍地英雄下夕烟。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 四言诗·祭母文\n",
            "\n",
            "呜呼吾母，遽然而死。寿五十三，生有七子。\n",
            "\n",
            "七子余三，即东民覃。其他不育，二女二男。\n",
            "\n",
            "育吾兄弟，艰辛备历。摧折作磨，因此遘疾。\n",
            "\n",
            "中间万万，皆伤心史。不忍卒书，待徐温吐。\n",
            "\n",
            "今则欲言，只有两端：一则盛德，一则恨偏。\n",
            "\n",
            "吾母高风，首推博爱。远近亲疏，一皆覆载。\n",
            "\n",
            "恺恻慈祥，感动庶汇。爱力所及，原本真诚。\n",
            "\n",
            "不作诳言，不存欺心。整饬成性，一丝不诡。\n",
            "\n",
            "手泽所经，皆有条理。头脑精密，擘理分情。\n",
            "\n",
            "事无遗算，物无遁形。洁净之风，传遍戚里。\n",
            "\n",
            "不染一尘，身心表里。五德荦荦，乃其大端。\n",
            "\n",
            "合其人格，如在上焉。恨偏所在，三纲之末。\n",
            "\n",
            "有志未伸，有求不获。精神痛苦，以此为卓。\n",
            "\n",
            "天乎人欤，倾地一角。次则儿辈，育之成行。\n",
            "\n",
            "如果未熟，介在青黄。病时揽手，酸心结肠。\n",
            "\n",
            "但呼儿辈，各务为良。又次所怀，好亲至爱。\n",
            "\n",
            "或属素恩，或多劳瘁。大小亲疏，均待报赍。\n",
            "\n",
            "总兹所述，盛德所辉。必秉悃忱，则效不违。\n",
            "\n",
            "致于所恨，必补遗缺。念兹在兹，此心不越。\n",
            "\n",
            "养育深恩，春辉朝霭。报之何时，精禽大海。\n",
            "\n",
            "呜呼吾母，母终未死。躯壳虽隳，灵则万古。\n",
            "\n",
            "有生一日，皆报恩时。有生一日，皆伴亲时。\n",
            "\n",
            "今也言长，时则苦短。惟挈大端，置其粗浅。\n",
            "\n",
            "此时家奠，尽此一觞。后有言陈，与日俱长。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 清平乐 蒋桂战争\n",
            "\n",
            "风云突变，\n",
            "\n",
            "军阀重开战。\n",
            "\n",
            "洒向人\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVxLO6kDdLl8"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJY9wNQ1ddDZ",
        "outputId": "dc8327c8-b998-4b31-d452-e7e485e53377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " #*AB·“”…　、。《》一丁七万丈三上下不与且世业丛东丝两严丧个中丰临丸丹为主丽举乃久么义之乌乎乐乔乘九也习乡书买乱乾了争事二于云五井亚些亟亡交亥亦产亩京亭亲人亿什仇今介仍从仑仓他仗付仙仞代令以仰仲任份仿伍伏休众会伟传伤伦伫伴伸似但位低住佐何佗余作你佳例供依侠侣侮侯侵便俊俏俘保信修俯俱倒倚倜借倦倭债倾假偏偕做停健偶偷偿傅傍傥催傲僧儿兀元兄充兆先光免兔党入全八公六兮兰共关兴兵其兹养兼冀内冈冉再冒冕写军农冠冢冬冯冰冲决况冷冻净凄准凉凋凌减几凡凤凭凯凰凶出击刀分切划列刘则刚初判利别到制刺前剑剧剩副割剿力劝办功加务劣动努劫劲劳势勇勤勾勿匈匕化北匝匡匪区医十千卅升午半华卑卒卓单卖南博卜占卢卧卫印危即却卵卷卿厂历厉压厚原去县参又及友双反发取受变叛叟叠口古句只叫叭可台史叶号司叹吃各合吉吊同名后吏吐向吓君吟否含听启吴吸吹吻吼吾呀呈告员呜周味呼命和咎咏咒咬咽哀响哎哥哨哪哭唇唐唤唯唱商啥啸啼善喇喊喋喑喜喝嗟嗡嘉嘶嚣囊四回因团园困围固国图囿圃圆土圣在地场均坏坐坑坚坠坤垂垄垒垓埃埋城域培基堂堆堑堕堤堪塔塘塞境墓墙增墟墨墩壁壑壤士壮声壳处备复夏夕外多夜大天太夫央失头夷夸夺奇奈奉奋奏奔奚奠女奴奸她好如妄妆妇妍妖妙妨妻始姐委姿威娄娆娇娓娘娥娴婢媚嫦嫩子孔字存孙季孤学孩孽宁宇守安宋完宏宗官宙定宜宝实宠客宣宫害宴宵家容宽宾宿寂寄密寇富寐寒寞寥寨寰寸对寺寻导寿封射将尊小少尔尘尚尤尧就尸尺尼尽屁层居屈屋展属屠履屦山岁岂岖岛岩岭岳岷岸岿峙峡峥峦峨峰峻崇崎崖崩嵘嶂嶷川州巡工巧巨巫己已巾市布帆师帐帜帝带席帷常幕干平年并幸幽广庄庆床庐应底庙府废度庭庶康廊廓廖延建开异弃弄弓引弟张弥弦弯弱弹强归当形彩彪彭影彻彼往征待徊律徐徒得徘御微德心必忆忍志忘忙忠忡忧快忱念忽怀态怅怎怒怕怜思急性怨怪总恋恍恐恙恨恩息恰恶恺恻悃悚患悬悲情惆惊惜惟惨惯想愁意愚感愤愿慈慌慧慨慷懈懿戈戎戏成我或战戚戟截戴户房所扇手才打托执扫扬扭扶批承技把抒抓投抗折抛报披抱抵抹抽拂拄担拍拒拖拘招拜拥拼拾持挂指挈按挖挟挡挥振挺挽捆捉捐损换捣捧捷掌排掣接推掩插揖握揽搅搏携摄摇摔摧摩撼擘擦攀收改攻放政故效敌救教敝敢散敦数整文斑斗料斜斥斧断斯新斶方施旁旅旋旌族旗无日旦旧旨早旭时旺昂昆昌明昏易昔星映春昨昭是晋晏晓晖晚晨普景晴晶智暂暑暖暗暮暴曙曦曰曲更曹曾最月有朋服朔朗望朝期木未末本朱朵机朽杀杂权李材村杖杜条来杨杭杯杰杷松板极枇枉析枕林果枝枪枯架柏染查柱柳标树株样核格栽桂桃桑桓桥梁梅梓梢梦械检棉棍棒棘棠森棹椒椰楚楫楼概榜槊槐横樯樱橘橙次欢欣欤欧欲欺款歇歌止正此步武歧死殊残殒段毁毅母每毒比毕毛氏民氓气氛氤氲水永汀求汇汉汗汛汝江池污汤汨汪汽沁沆沉沙没沦沧沫河沸治沽沾泄泉泓法波泣泥注泪泱泳泽洁洋洒洗洛洞洢津洪洲活派流浅浆浊测济浓浣浦浩浪浮浴海浸涂消涌涎涔涛涤润涨涯液涸涿淑淘淡深淹添清渊渔渝渡温渭游渺湖湘湾湿溉源溜溟溢溪溯溶滂滋滑滔滚满滨滩滴演漠漫潇潜潭潮潺澄澜激濯瀛火灭灯灰灵灾炉炊炎炬炮炸点炼烂烈烛烟烧烬热烹烽焉焚焰然煎煞煤煦照煮熊熟燃燎燕爆爪爱父爹爽片版牙牛牡牢物牲牺犬状犹狂狐独狮狼猎猖猛猜猪猴猿玉王环现玲珍珠球琅理琉琳琴琼瑜瑟瑶瓒瓜瓦瓯甘甚生用甫田由甲电男画界畔留略番疆疏疑疮疾病痍痕痛瘁瘟登白百的皆皇皋盆盈益监盗盘盛盟目直相盾省眉看真眠眶眺眼着督睿瞻瞽矛矢矣知短石砥破础硬碌碎碑碣碧碰磅磨礴示社祀祖祝神祥票祭祸禁离禽秀秉秋种秕秘租秦积称移秽稀稊程稍税稔稠稻穆穴穷穹空穿突窗窥立竞竟章竦端竹竿笑笔笛笞第笼等答筹算管篆篇簧米类粉粒粗粤粪粮粱精糠素索紧紫累絮繁红纤约级纪纭纲纵纶纷纸线练组绅细织终经结绕绘给绝统继绩续绳维绵绽绿缕编缘缚缨缸缺网罗罢罪置罴羊美羞羡群羽翁翅翔翘翠翥翩翰翻翼老者而耐耕耸耻耿聋联聚聪肃肇肉肝肠肥肯育胄胆背胜胡胥胳胸能脂脑脚腐腰腾膊膏臂自至致舌舍舒舜舞舟航舰舸船艟艨良艰色艾节芒芙花芳苍苏苔苗苛若苦英茂茅茏茜茨茫茶荆草荐荒荔荡荣荦药莫莱莲获莺莽菊菌菜菩菰菽萋营萦萧萨落葱蒋蒙蒲蒸蓉蓝蓬蔑蔚蕡薄薇薜藏虎虏虑虞虫虹虽蚀蚁蚂蚊蚍蚩蛇蛙蛟蛮蜀蜉蜡蜣蜮蝇蝣蝶螳蠢血行衍衔街衡衣补表袋袍袖被裁裂装裔裹西要覃覆见观觉角觞解触言誉誓警计订认讨让训议讯记讲讶许论讼诀证评识诉试诗诚话诡该语误诳说诸读谁调谈谊谋谒谓谖谢谣谪谷豆豚象豪豫豹貌负责贤败账货贫贰贱贵贺贼贾赃资赋赍赏赔赖赚赞赠赣赤赫走赴赶起趁越趋趣足跃跌跖跤路跹踊踏踞踟踪踯蹄蹇蹈蹉蹰蹻躅身躯车转轮轻载辈辉辍输辙辛辞辟辩辰辱边辽达迁迅过迈迎运近返还这进远违连迟迤迫述迴迷迹追退送适逆逊透逐途通逝逞造逢逶逸逼遁遇遍遏遐遒道遗遘遣遥遮遵遽那邦邪邯邻郁郊郎郡部郭郸都鄂酒酣酬酸酹酿醉醒采里重野量金鉴鏖针钊钓钟钢钧钱钺铁铃铐铓铜铢铩铭铲银铺锁锄锋锐锤锦锷镇镜镝镣镰长门问闲间闹闻闽阀阁阅阎阑阒阔阗阜队防阳阴阵阶附际陆陇陈降限陕除险陵陶陷隆隈随隘隳隶难雀雁雄雌雕雨雪雳零雷雹雾需霄霆震霜霞霭露霸霹青静非靠面革鞋鞍鞭韩音韵韶页项顺须顾顿颁颂颅领颇颐频题颜颠风飏飒飘飙飞食餐饕饥饬饭饮饱饷饿馀首香馨马驰驱驻驾骂骄骋骐骗骚骤骥骨骸高鬓鬼魁魂魅魏魔鱮鱼鲁鲂鲋鲜鲲鲸鳖鷃鸟鸡鸣鸦鸿鹏鹤鹫鹰鹿麦黄黍黑鼎鼓鼠齐齿龙龟！（），：；？\n",
            "2106\n"
          ]
        }
      ],
      "source": [
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdqOWRzPdl0z"
      },
      "outputs": [],
      "source": [
        "stoi = {char:idx for idx, char in enumerate(chars)}\n",
        "itos = {idx: char for idx, char in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abYw3i2heML7"
      },
      "outputs": [],
      "source": [
        "# print(stoi['北'])\n",
        "# print(itos[273])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyiU6MpTelc3"
      },
      "outputs": [],
      "source": [
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda ids: ''.join([itos[i] for i in ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfIrIOn6fBQu",
        "outputId": "0877bf93-2917-4c40-d898-13ad2cfd6396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[273, 415, 2025, 176]\n",
            "北国风光\n"
          ]
        }
      ],
      "source": [
        "# 毛爷爷的诗\n",
        "print(encode('北国风光'))\n",
        "print(decode([273, 415, 2025, 176]))\n",
        "\n",
        "# shakespeare poetry\n",
        "# print(encode(\"hii there\"))\n",
        "# print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmhgBPN7f6xj",
        "outputId": "d378fc5c-d2a1-4a1b-f259-39ce12b098a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16498]) torch.int64\n",
            "tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865, 1772,   47,    0,    0,\n",
            "         203,  330,  628,  204, 2001,  371, 2102,    0,    0,  899,  336, 1930,\n",
            "         231,  887,  479,   12,    0,    0,  275,  657,   15,  642,   23,  158,\n",
            "        1305, 2102,    0,    0, 1679,  349, 1193, 1170, 1335, 1828,   12,    0,\n",
            "           0,  423,   41, 1894, 1894,  308, 1834, 2102,    0,    0,  204, 1067,\n",
            "          34,   34,  344,   85,   12,    0,    0, 1385,  865,  907, 1571,  937,\n",
            "          67,  758, 2102,    0,    0, 1995, 1981,   15,  463,  938,  260,   12,\n",
            "           0,    0,    0,    0,    0,    2,    1, 1609, 1616, 1649,    6, 2089,\n",
            "        2084, 1027,    0,    0, 1589, 1589,   54, 1125, 1126,   35,  415, 2102,\n",
            "           0,    0, 1091, 1091,   15, 1463, 1408,  291,  273,   12,    0,    0,\n",
            "        1220, 1979, 1605, 1576, 1576, 2102,    0,    0, 2098, 1646, 1919,  473,\n",
            "        1082,   12,    0,    0, 2089, 2084, 1354,  123,  311, 2105,    0,    0,\n",
            "         249,  948, 1167,   82,  465,   12,    0,    0,  798, 1884, 1888, 1184,\n",
            "        1184, 2102,    0,    0,  707, 1196, 1846, 1136, 2060, 2099,    0,    0,\n",
            "           0,    0,    0,    2,    1,   17,  699,    6,  372,  996,   70,  521,\n",
            "         175, 1294,    0,    0, 2037, 1590, 1439, 1139,  957, 1542,  712, 2102,\n",
            "        1446,  328, 1163,  626,  335, 1048, 2089,   12,    0,    0,   20,  279,\n",
            "          15,  649, 1826,  903,  415, 2102, 1617, 1574,  907, 1571, 1723,  285,\n",
            "        1415,   12,    0,    0, 1252, 2055,  475, 1332, 1949, 1530,  888, 2102,\n",
            "        2025, 1253, 1931,  542,  868, 1345, 1896,   12,    0,    0, 1600, 1862,\n",
            "         910,  912, 1083, 1073, 1127, 2102, 1683, 2069, 1537, 1820,  561,  918,\n",
            "        1082,   12,    0,    0,    0,    0,    0,    2,    1,   17,  699,    6,\n",
            "         207,   67,    0,    0, 1980,  308,  207,   67, 1320, 1450, 2030, 2102,\n",
            "          18, 1574, 1461, 1732,   15,  907, 1395,   12,    0,    0, 2060,  474,\n",
            "        1185, 1185,  563, 1126,  729, 2102,  473,  423,  705,  705,  935, 1069,\n",
            "         359,   12,    0,    0, 1261,  948, 1583, 1976, 2047, 1633, 1741, 2102,\n",
            "         943,  900, 1739,  977,  726, 1236, 1497,   12,    0,    0, 1010, 1574,\n",
            "        1038,  399, 1192,  474, 1980, 2102,  214, 1053, 1576, 1655,  957, 1777,\n",
            "         483,   12,    0,    0,    0,    0,    0,    2,    1,   17,  699,    6,\n",
            "         243, 2008,  600,    0,    0,  242, 1013,  131, 1395,  375, 1849,  625,\n",
            "        2102,  870,  411,   20,  279,   65,  649,  246,   12,    0,    0, 1452,\n",
            "         899,  303, 1772,  204,  492,  779, 2102, 2091,  786, 2060,  748, 1994,\n",
            "          41, 2004,   12,    0,    0,   40,  948, 1255, 1254,  471,  462,  711,\n",
            "        2102,  876,  874,  901,  947,  839,  890,  474,   12,    0,    0,  399,\n",
            "        1340, 1402, 1611,  280, 1894, 1136, 2102, 1858,  423, 1583, 1976,   22,\n",
            "         469, 1220,   12,    0,    0,    0,    0,    0,    2,    1,  407, 1689,\n",
            "        1713,    6, 1378, 1060,  881,    0,    0,  367,  370,  362, 1060, 2102,\n",
            "        1869, 1229, 1516, 1053,   12,  573,   68,  279,   20, 2102, 1294,  948,\n",
            "          17,  521,   12,    0,    0,   17,  521,  125,   20, 2102,  300,   29,\n",
            "        1067, 1680,   12,  191,   92,   23, 1533, 2102,   65,  491,   65, 1301,\n",
            "          12,    0,    0, 1533,  362,  172,  678, 2102, 1568, 1809,  466,  306,\n",
            "          12,  859,  803,  126, 1368, 2102,  409, 1049, 1864, 1312,   12,    0,\n",
            "           0,   35, 1935,   18,   18, 2102, 1323,  112,  707,  334,   12,   23,\n",
            "         710,  287,   58, 2102,  697,  700, 1165,  348,   12,    0,    0,   86,\n",
            "         237, 1042, 1689, 2102,  329,  948,   31, 1417, 2103,   15,  237, 1332,\n",
            "         706, 2102,   15,  237,  738,  155,   12,    0,    0,  362, 1060, 2060,\n",
            "        2025, 2102, 2042,  847,  292, 1243,   12, 1829, 1824,   81, 1309, 2102,\n",
            "          15, 1323, 1681, 1803,   12,    0,    0,  743,  744,  764, 1376, 2102,\n",
            "         761,  260,  665, 1077,   12, 1243,  253,  784,  315, 2102,  310,  959,\n",
            "        1341, 1714,   12,    0,    0,   23,  126, 1720, 1689, 2102,   23,  524,\n",
            "        1043,  707,   12,  880, 2035,  774,  730, 2102,   15,   30,   23, 1716,\n",
            "          12,    0,    0,  786, 1113,  784, 1470, 2102, 1323,  948,  972, 1280,\n",
            "          12,  479, 1544, 1443,  559, 2102,  862, 1280,  232,  750,   12,    0,\n",
            "           0,   64,  900, 1863, 1429, 2102, 1253,  900, 1856,  688,   12, 1114,\n",
            "         215,   48, 2025, 2102,  111, 1858,  778, 1893,   12,    0,    0,   23,\n",
            "         993,   15,  581, 2102, 1797,  707, 1668, 1893,   12,   68,  706, 1598,\n",
            "        1598, 2102,   44,  191,  473, 1417,   12,    0,    0,  341,  191,   82,\n",
            "        1002, 2102,  496,  422,   21, 1226,   12,  738,  155,  784,  422, 2102,\n",
            "          20, 1458,   48,  958,   12,    0,    0,  948,  711,  957,  116, 2102,\n",
            "         948, 1076,   23, 1603,   12, 1443, 1375, 1316, 1582, 2102,   99, 1049,\n",
            "          40,  288,   12,    0,    0,  474,   50,   82, 1040, 2102,  153,  423,\n",
            "          15, 1685,   12, 1037,  237,  169, 1804, 2102, 1533,   48,  774, 1661,\n",
            "          12,    0,    0,  496,  987,  957, 1237, 2102,   87,  422, 1996, 2089,\n",
            "          12, 1313,  907,  852,  786, 2102, 1887,  707, 1471, 1530,   12,    0,\n",
            "           0,  118,  370,  169, 1804, 2102,  340,  258,   40, 1567,   12,  314,\n",
            "        1037,  784,  721, 2102,  495,   81, 1553, 1243,   12,    0,    0,  776,\n",
            "         596, 1445,  739, 2102,  776,  471,  264, 1317,   12,  473,  578,   81,\n",
            "        1309, 2102,  425,  697,  805, 1759,   12,    0,    0,  733,  192,  784,\n",
            "        1835, 2102, 1332,  706,  784, 1805,   12,  708, 1384,  745,  718, 2102,\n",
            "         237,  871,   23, 1830,   12,    0,    0, 1554,   66,  784,  738, 2102,\n",
            "         708, 1667, 1863, 1491,   12,  719,  192,  422,  192, 2102, 1049,  707,\n",
            "          23, 1774,   12,    0,    0,  193, 1533, 1157,  739, 2102,  918, 1805,\n",
            "         954, 1992,   12,  805,   48,  123,  907, 2102, 1443, 1382,  473, 1139,\n",
            "          12,    0,    0,  367,  370,  362, 1060, 2102, 1060, 1469,  957, 1053,\n",
            "          12, 1798,  464, 1639, 1971, 2102, 1207,  237,   18,  327,   12,    0,\n",
            "           0,  948, 1294,   15,  901, 2102, 1323,  805,  739,  907,   12,  948,\n",
            "        1294,   15,  901, 2102, 1323,  115,   81,  907,   12,    0,    0,   86,\n",
            "          55, 1689, 1931, 2102,  907,  237, 1582, 1355,   12,  754,  826,  473,\n",
            "        1417, 2102, 1496,  191, 1438, 1127,   12,    0,    0, 1049,  907,  552,\n",
            "         490, 2102,  589, 1049,   15, 1686,   12,  346,  948, 1689, 1958, 2102,\n",
            "          24,  901,  145, 1931,   12,    0,    0,    0,    0,    0,    2,    1,\n",
            "        1160,  648,   51,    1, 1619, 1004,  777,   63,    0,    0, 2025,   67,\n",
            "        1409,  322, 2102,    0,    0,  203, 1939, 1894,  672,  777,   12,    0,\n",
            "           0, 1116,  349,   82])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgUXhVtUgptd"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "valid_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6581gFPhhJN",
        "outputId": "57219e0d-4cb1-48a7-cf05-4b9acbad57f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865, 1772])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBHL5NiQhznJ",
        "outputId": "7ff617f6-c800-4c49-8bf2-4e2dd2eb3175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when context is tensor([2]), target is 1\n",
            "when context is tensor([2, 1]), target is 1678\n",
            "when context is tensor([   2,    1, 1678]), target is 1082\n",
            "when context is tensor([   2,    1, 1678, 1082]), target is 947\n",
            "when context is tensor([   2,    1, 1678, 1082,  947]), target is 6\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6]), target is 1385\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6, 1385]), target is 865\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865]), target is 1772\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for i in range(block_size):\n",
        "    context = x[:i+1]\n",
        "    target = y[i]\n",
        "    print(f\"when context is {context}, target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3YGZh4vk1bh",
        "outputId": "b3ae7923-c5cc-430b-95e7-b7736da972d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  17, 1456, 1393,  600,  711, 2102,    0,    0],\n",
            "        [2102,    0,    0, 1678, 2025, 1192,  303,  527],\n",
            "        [ 111, 1858,  778, 1893,   12,    0,    0,   23],\n",
            "        [2042,    6,  866, 1538,   52,  956,   13,  552]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[1456, 1393,  600,  711, 2102,    0,    0,  710],\n",
            "        [   0,    0, 1678, 2025, 1192,  303,  527,  438],\n",
            "        [1858,  778, 1893,   12,    0,    0,   23,  993],\n",
            "        [   6,  866, 1538,   52,  956,   13,  552,   58]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else valid_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# print(get_batch('train'))\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaRE6r7JpaTM",
        "outputId": "22319b92-1111-4c98-f968-9472e776523d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when context is tensor([17]), target is 1456\n",
            "when context is tensor([  17, 1456]), target is 1393\n",
            "when context is tensor([  17, 1456, 1393]), target is 600\n",
            "when context is tensor([  17, 1456, 1393,  600]), target is 711\n",
            "when context is tensor([  17, 1456, 1393,  600,  711]), target is 2102\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102]), target is 0\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102,    0]), target is 0\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102,    0,    0]), target is 710\n",
            "when context is tensor([2102]), target is 0\n",
            "when context is tensor([2102,    0]), target is 0\n",
            "when context is tensor([2102,    0,    0]), target is 1678\n",
            "when context is tensor([2102,    0,    0, 1678]), target is 2025\n",
            "when context is tensor([2102,    0,    0, 1678, 2025]), target is 1192\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192]), target is 303\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192,  303]), target is 527\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192,  303,  527]), target is 438\n",
            "when context is tensor([111]), target is 1858\n",
            "when context is tensor([ 111, 1858]), target is 778\n",
            "when context is tensor([ 111, 1858,  778]), target is 1893\n",
            "when context is tensor([ 111, 1858,  778, 1893]), target is 12\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12]), target is 0\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0]), target is 0\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0,    0]), target is 23\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0,    0,   23]), target is 993\n",
            "when context is tensor([2042]), target is 6\n",
            "when context is tensor([2042,    6]), target is 866\n",
            "when context is tensor([2042,    6,  866]), target is 1538\n",
            "when context is tensor([2042,    6,  866, 1538]), target is 52\n",
            "when context is tensor([2042,    6,  866, 1538,   52]), target is 956\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956]), target is 13\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956,   13]), target is 552\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956,   13,  552]), target is 58\n"
          ]
        }
      ],
      "source": [
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f'when context is {context}, target is {target}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQHyc87uqAFQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # 这里是有讲究的，这个embedding就是通过当前词预测下一个词，这是bigram吗？怎么感觉是unigram\n",
        "        # 没问题，就是bigram。uniqgram就是每个词的出现只与它自己有关，bigram是每个词的出现与它的前一个有关\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4XwzL8uuAKp",
        "outputId": "ecdaf46f-b4fd-4279-967d-a0e08c90c4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2106])\n",
            "tensor(7.9996, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "滂东分赞项批孙单蹉泄摔壮聋答方椰勿诉冬苦簧身鲜印育指景素亥莱望租古第奉谣锄卢岖装逝况守辞蒸转解诳误呼料恍误久卷妙辛昌登怕覆执渔因宾问桑忡日见色这镜壮呀鹤雄携入卒酒儿依园妻议必今皆土裹应房薄民跹荣鲁瀛明\n"
          ]
        }
      ],
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Quc7QXnNMl"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xSxhHoROy5f",
        "outputId": "954f002a-4b54-412f-fda5-08f07c5cbd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 8.227705001831055, valid loss: 8.179295539855957\n",
            "train loss: 8.062766075134277, valid loss: 8.039710998535156\n",
            "train loss: 7.964939117431641, valid loss: 7.894080638885498\n",
            "train loss: 7.80561637878418, valid loss: 7.877248764038086\n",
            "train loss: 7.787447452545166, valid loss: 7.823698043823242\n",
            "train loss: 7.757097244262695, valid loss: 7.7348713874816895\n",
            "train loss: 7.620429992675781, valid loss: 7.706653118133545\n",
            "train loss: 7.619467258453369, valid loss: 7.729081630706787\n",
            "train loss: 7.501373291015625, valid loss: 7.591119289398193\n",
            "train loss: 7.45088529586792, valid loss: 7.756300449371338\n",
            "train loss: 7.438085079193115, valid loss: 7.496287822723389\n",
            "train loss: 7.383625507354736, valid loss: 7.573101043701172\n",
            "train loss: 7.329615116119385, valid loss: 7.4824652671813965\n",
            "train loss: 7.077822685241699, valid loss: 7.313206195831299\n",
            "train loss: 7.02650260925293, valid loss: 7.385712623596191\n",
            "train loss: 6.857977390289307, valid loss: 7.538801193237305\n",
            "train loss: 6.895392894744873, valid loss: 7.1207275390625\n",
            "train loss: 6.699440956115723, valid loss: 7.2885661125183105\n",
            "train loss: 6.875024795532227, valid loss: 7.0616536140441895\n",
            "train loss: 6.6329803466796875, valid loss: 7.156210899353027\n",
            "train loss: 6.663938045501709, valid loss: 7.086459636688232\n",
            "train loss: 6.56096887588501, valid loss: 7.106273651123047\n",
            "train loss: 6.431014537811279, valid loss: 6.97570276260376\n",
            "train loss: 6.250826835632324, valid loss: 6.7476277351379395\n",
            "train loss: 6.295362949371338, valid loss: 6.723360061645508\n",
            "train loss: 6.275694370269775, valid loss: 7.148281574249268\n",
            "train loss: 6.202600002288818, valid loss: 6.935051918029785\n",
            "train loss: 6.26876163482666, valid loss: 6.810391426086426\n",
            "train loss: 6.26568078994751, valid loss: 7.082875728607178\n",
            "train loss: 6.06411600112915, valid loss: 6.582479000091553\n",
            "train loss: 5.773815631866455, valid loss: 6.758648872375488\n",
            "train loss: 5.853133201599121, valid loss: 6.717687606811523\n",
            "train loss: 5.9872660636901855, valid loss: 6.653452396392822\n",
            "train loss: 5.825678825378418, valid loss: 6.5532708168029785\n",
            "train loss: 5.780938625335693, valid loss: 6.424733638763428\n",
            "train loss: 5.665343284606934, valid loss: 6.6306681632995605\n",
            "train loss: 5.8054986000061035, valid loss: 6.657829761505127\n",
            "train loss: 5.463012218475342, valid loss: 6.667204856872559\n",
            "train loss: 5.294428825378418, valid loss: 6.611881256103516\n",
            "train loss: 5.2645344734191895, valid loss: 6.415426254272461\n",
            "train loss: 5.605679035186768, valid loss: 6.284614562988281\n",
            "train loss: 5.264440059661865, valid loss: 6.685435771942139\n",
            "train loss: 5.450766563415527, valid loss: 6.801301002502441\n",
            "train loss: 5.350837230682373, valid loss: 6.241654396057129\n",
            "train loss: 5.18683385848999, valid loss: 6.411829948425293\n",
            "train loss: 5.1158294677734375, valid loss: 6.581803321838379\n",
            "train loss: 5.08320951461792, valid loss: 6.249319076538086\n",
            "train loss: 5.0555267333984375, valid loss: 5.680738925933838\n",
            "train loss: 4.8838982582092285, valid loss: 6.3776535987854\n",
            "train loss: 4.778415679931641, valid loss: 6.331445217132568\n",
            "train loss: 4.681112289428711, valid loss: 6.476998805999756\n",
            "4.681112289428711\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_iter = 5000\n",
        "\n",
        "for step in range(max_iter): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    if step % 100 == 0 or step == max_iter - 1:\n",
        "        x_val, y_val = get_batch('valid')\n",
        "        logits_val, loss_val = m(x_val, y_val)\n",
        "\n",
        "        print(f'train loss: {loss}, valid loss: {loss_val}')\n",
        "\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 基本到2.2 就下不去了\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpMJqXjCjU6i",
        "outputId": "43674796-8eab-42cf-9e98-8dd1028833f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "torch.zeros(1, 1, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQYMv0aojmNs",
        "outputId": "cd03de7e-eea3-4aa3-a0a2-ae34f2ee62ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# gen_idx = torch.tensor([encode('北国风光')], dtype=torch.long)\n",
        "gen_idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "gen_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC3d8-PoO5mj",
        "outputId": "9710c14a-9887-412b-e90f-1135e4bfbb82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "胜理尘妨呼槊际罪割涯截畔道原诗苔帷徒带血怕贰元我湿夸皋浓赖宇识辰奚鸟适营豆滩度民铜民伴斑闹廊顾瑶偷赖：省睿汝她滚两市晖曰甫鱼织割稍训凯始艰但动林快翠世事好文王俏樱叹珠拂南京静若畔空爱猜飒瑟朝驱领烹肠棉徊困惆许纲谣良汤林疏征去骂赚满趋目青霜爹（原留欤炉斧晓恩肃荆蕡鼎讼黍抵遘棠极梅死极芒吐乎编饕昭素遥男施椰叹灰稊彪卵邦碌秦螳秦蹻法雁涿弓蜉静丧启继奏入例命棉赤）晏孩风旺钊七测挡慷蚊厉辰然羊藏痍抹宇他隆池堂漫竹阎恐驰猖按隶枉队剑源渭香朝阑沉玉必事诀疏茏宜办精行凌账涂幽但贰乘引扭免伦誓所叠床县骗矛面统峰么其四卫儿础冯帆想厚逶唯静市罪原垓紧寇白罗布抵限冯南槐执庭驻胄堑涤贱肥水域绝陷难翁移路龙叫辉续朱仲起鸟早变翁叛山关仿铜忧忠顺聚四嶷闻位？玲跹效样关结猎飒在寰荦曦本绕其标沁弥长林除诳驱蜀瞻骋莱泳弟通严蚍锤折腰评别欧编鹫凰抽疑征煎证庐切扬恩应嫩搅翔粉微鼠寨死绝堤炼祀床军反仑茫顺篆溪仇昏振镜杖滴围壤愿悃浦十入丹畔琅闹兀北得踏问公十剿覆樱醒远遥讯森罢乌拍量溉却液楚鲜讲艾堂离宜及赣话慨启远尊腾猪租频索直洞招邯滂兔阅母韶况领结并煞角灵督计卅弯贫凯铜虹死谓安在细涔口接攀祝崇就楼赤士司烽供组莺冯已睿煎掣忆生柏耕吻\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = gen_idx, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# attention的数学技巧"
      ],
      "metadata": {
        "id": "T7Lsk5R91v2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# 下三角矩阵就能做到和左边的词元attention\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "# 但是这里我们要模拟的不是mask，而是一个K * Q得到的attention权重矩阵，还得归一化\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('---')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('c=')\n",
        "print('---')\n",
        "print(c)\n",
        "\n",
        "# c look as the column combination of a\n",
        "# 2.  0. 0  2\n",
        "# 1.  3. 0. 4\n",
        "# 2/3 2  2. 4.6666\n",
        "\n",
        "# 但是这里应该看成 row combination of b\n",
        "# 1.0 * [2., 7.] + 0.0 * [6., 4.] + 0.0 * [6., 5.] 这就是第1个token的BOW表示\n",
        "# 0.5 * [2., 7.] + 0.5 * [6., 4.] + 0.0 * [6., 5.] 这就是第2个token的BOW表示\n",
        "# 0.3 * [2., 7.] + 0.3 * [6., 4.] + 0.3 * [6., 5.] 这就是第3个token的BOW表示\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCIKXHxu118W",
        "outputId": "7446f31e-815a-4c8d-fc68-705a157eb577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "---\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO7q8nUBcYAQ",
        "outputId": "6d750883-f706-4381-ff5b-2e50f64f0224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 先用朴素BOW来表达？\n",
        "# x[b, t] = mean_{i <= t} x[b, i]\n",
        "\n",
        "xbow = torch.zeros(B, T, C)\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        # 私以为比Andrej老师的变量名要好，因为这里也包含了step t这个token\n",
        "        x_t_tprev = x[b, :t+1] # shape (t, C)\n",
        "        xbow[b, t] = torch.mean(x_t_tprev, 0)\n",
        "\n",
        "print(xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuKWNTK8gIf_",
        "outputId": "e511661c-431f-4069-808c-3d97019cc9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 再用上面那个tril的矩阵乘法来表示\n",
        "# 为什么用tril？因为不用写for循环，并行度更高\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "print('wei_tril:')\n",
        "print(wei)\n",
        "\n",
        "\n",
        "# 这里keepdim=True shape是(8, 1)所以除法就是每行除以对应的那一行的1个sum出来的数\n",
        "# keepdim=False, shape是(8, ), 那就是每一行从左到右每个元素都找到对应的去除\n",
        "wei_sum = wei.sum(1, keepdim=True)\n",
        "\n",
        "print('wei_sum:')\n",
        "print(wei_sum)\n",
        "wei = wei / wei_sum\n",
        "\n",
        "\n",
        "print('wei:')\n",
        "print(wei)\n",
        "\n",
        "xbow2 = wei @ x\n",
        "\n",
        "# 发现太小的值，这里[1,5,1] 是 0.0020，会导致allclose判断为False，\n",
        "# 这个问题很有意思，我记得21年跑这个代码没有这个问题，现在出现了。我也运行了一下Andrej的原始notebook也是Fasle\n",
        "# 说明大概率是python和torch的版本升级导致的，可能是seed rand的数字不一样了\n",
        "print('xbow2:')\n",
        "print(xbow2[1][5][1])\n",
        "print(xbow2[1][5][1] - xbow[1][5][1])\n",
        "torch.allclose(xbow[1][5][1], xbow2[1][5][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K30_1hzjUhb",
        "outputId": "156aedd0-57b3-44e9-a850-2f3e430d0a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei_tril:\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei_sum:\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.],\n",
            "        [5.],\n",
            "        [6.],\n",
            "        [7.],\n",
            "        [8.]])\n",
            "wei:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "xbow2:\n",
            "tensor(0.0020)\n",
            "tensor(-3.2363e-08)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 用softmax\n",
        "# 为什么要用softmax？后续如果是模型过完一些层，得出来的是logits而不是probs\n",
        "# logits范围(-inf, inf)，而不是像上面都是类似probs的 1/n 范围[0.0, 1.0]\n",
        "# 过一下softmax，范围就到[0.0, 1.0]了\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = torch.where(wei == 0.0, float('-inf'), wei)\n",
        "\n",
        "\"\"\"\n",
        "在PyTorch中，`F.softmax` 函数用于沿着指定的维度对输入张量进行softmax操作。对于张量 `wei`，其形状为 `(T, T)`，让我们分析一下 `F.softmax(wei, 1)` 和 `F.softmax(wei, -1)` 的含义：\n",
        "\n",
        "1. **`F.softmax(wei, 1)`**:\n",
        "   - 这里的 `1` 表示在第1个维度（即列方向）上进行softmax操作。对于每一行，`F.softmax` 会计算该行内的softmax值。\n",
        "\n",
        "2. **`F.softmax(wei, -1)`**:\n",
        "   - 这里的 `-1` 表示在最后一个维度上进行softmax操作。对于二维张量 `(T, T)` 来说，最后一个维度也是列方向（即维度1）。因此，`F.softmax(wei, -1)` 实际上与 `F.softmax(wei, 1)` 是等价的。\n",
        "\n",
        "总结来说，`F.softmax(wei, 1)` 和 `F.softmax(wei, -1)` 在形状为 `(T, T)` 的张量上执行时，是相同的操作，因为它们都在列方向上计算softmax。\n",
        "\"\"\"\n",
        "wei = F.softmax(wei, 1)\n",
        "print('wei:')\n",
        "print(wei)\n",
        "\n",
        "xbow3 = wei @ x\n",
        "\n",
        "# xbow2和xbow3是一样的\n",
        "# 因为xbow2和xbow3都是先除再加，xbow的mean是先加再除\n",
        "torch.allclose(xbow2, xbow3)"
      ],
      "metadata": {
        "id": "55UHB8xEqJEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6aeae9-b6ae-4c6e-b3a7-c3fbed6ad6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 终于来了！自注意力self-attention\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 2, 16, 8\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "header_size = 4\n",
        "\n",
        "key = nn.Linear(C, header_size)\n",
        "query = nn.Linear(C, header_size)\n",
        "value = nn.Linear(C, header_size)\n",
        "\n",
        "# k,q,v shape (B, T, head_size)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "v = value(x)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = q @ k.transpose(1, 2) # shape (B, T, T)\n",
        "\n",
        "# 这里wei和tril的shape不一样，看来是能自动boardcast的\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, -1)\n",
        "\n",
        "x_attent_left = wei @ v\n",
        "\n",
        "x_attent_left.shape # shape (B, T, header_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVdYLk_4eDFv",
        "outputId": "017a2a0c-6a4d-495c-e84c-b10e4a4f78fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 16, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意：\n",
        "- 注意力是一种**交流机制**。可以看成是有向图中的节点互相关注，并将所有指向它们的节点的信息加权汇总，权重取决于数据。\n",
        "- 没有空间概念。注意力只是作用于一组向量。这就是我们需要对标记进行位置编码的原因。\n",
        "- 在 \"编码器 \"注意力区块中，只需删除用下三角矩阵进行屏蔽的一行，就可以让所有标记进行交流。这里的区块被称为 \"解码器 \"注意区块，因为它具有下三角形掩码，通常用于自回归设置，如语言建模。\n",
        "- \"自注意力\"只是指键和值与查询的来源相同。在 \"交叉注意力\"中，查询仍由 x 生成，但键和值来自其他外部来源（如编码器模块）。\n",
        "- \"缩放 \"注意力额外将 wei 除以 1/sqrt（head_size）。这样，当输入 Q、K 为单位方差时，wei 也将是单位方差，Softmax 将保持分散，不会过于饱和。说明如下"
      ],
      "metadata": {
        "id": "SSe2mDWXFT5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(8, 4) # shape (T, head_size)\n",
        "print(f'q var: {q.var()}')\n",
        "\n",
        "k = torch.randn(8, 4) # shape (T, head_size)\n",
        "print(f'k var: {k.var()}')\n",
        "\n",
        "wei = q @ k.transpose(0, 1) # shape (T, T)\n",
        "print(f'wei var: {wei.var()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6crEe9MUGsSq",
        "outputId": "2ca782a6-2977-41b6-b971-50671917ea7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q var: 0.55227130651474\n",
            "k var: 1.0105643272399902\n",
            "wei var: 2.630138874053955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei_norm = wei / (4 ** 1/2) # head_size is 4\n",
        "\n",
        "print(f'wei_norm vars: {wei_norm.var()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny4e6uSrH4IB",
        "outputId": "213143c2-1d02-4168-a9f7-70208a7cb638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei_norm vars: 0.6575347185134888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 方差大了过完softmax会变得过于峰值(peak)\n",
        "\n",
        "# 设置打印选项，不使用科学计数法\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "wei_probs = F.softmax(wei, dim=1)\n",
        "print(f'wei_probs: {wei_probs}') # 可以看到[3, 3] 不用科学计数法都是0.0000了，这样整个向量会趋向于one-hot\n",
        "# one-hot有什么不好呢？个人的想法是这里wei是每个token和其他token的关系权重，两两之间总归是有一些关系的\n",
        "\n",
        "wei_norm_probs = F.softmax(wei_norm, dim=1)\n",
        "print(f'wei_norm_probs: {wei_norm_probs}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0bQUBvKIZoS",
        "outputId": "65e15394-5034-4c42-fed0-fe54261d3897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei_probs: tensor([[    0.0127,     0.1197,     0.0321,     0.2054,     0.0137,     0.5546,\n",
            "             0.0208,     0.0410],\n",
            "        [    0.0285,     0.0135,     0.0004,     0.1478,     0.0122,     0.0009,\n",
            "             0.1557,     0.6410],\n",
            "        [    0.0442,     0.0271,     0.0717,     0.0142,     0.6493,     0.0171,\n",
            "             0.1586,     0.0178],\n",
            "        [    0.0960,     0.0813,     0.5371,     0.0106,     0.0814,     0.1663,\n",
            "             0.0118,     0.0155],\n",
            "        [    0.0609,     0.0431,     0.0761,     0.1198,     0.1977,     0.0493,\n",
            "             0.4034,     0.0498],\n",
            "        [    0.3244,     0.0644,     0.2650,     0.0174,     0.1623,     0.0263,\n",
            "             0.0584,     0.0818],\n",
            "        [    0.0924,     0.0186,     0.0485,     0.0088,     0.6168,     0.0040,\n",
            "             0.1757,     0.0353],\n",
            "        [    0.3909,     0.0708,     0.1346,     0.0087,     0.2104,     0.0115,\n",
            "             0.0322,     0.1409]])\n",
            "wei_norm_probs: tensor([[0.0491, 0.1505, 0.0779, 0.1971, 0.0508, 0.3239, 0.0627, 0.0880],\n",
            "        [0.0835, 0.0575, 0.0093, 0.1900, 0.0546, 0.0146, 0.1950, 0.3956],\n",
            "        [0.0943, 0.0738, 0.1201, 0.0535, 0.3613, 0.0586, 0.1786, 0.0598],\n",
            "        [0.1315, 0.1210, 0.3109, 0.0437, 0.1210, 0.1730, 0.0461, 0.0529],\n",
            "        [0.0949, 0.0798, 0.1061, 0.1331, 0.1709, 0.0853, 0.2442, 0.0858],\n",
            "        [0.2223, 0.0990, 0.2009, 0.0514, 0.1572, 0.0633, 0.0943, 0.1116],\n",
            "        [0.1376, 0.0618, 0.0996, 0.0425, 0.3554, 0.0285, 0.1897, 0.0850],\n",
            "        [0.2529, 0.1076, 0.1484, 0.0376, 0.1856, 0.0434, 0.0726, 0.1518]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 完整的实现GPT"
      ],
      "metadata": {
        "id": "ifRteA59NNct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    def __init__(self, config):\n",
        "        self.B = config['B']\n",
        "        self.T = config['T']\n",
        "        self.C = config['C']\n",
        "\n",
        "        self.vocab_size = config['vocab_size']\n",
        "\n",
        "        self.header_size = config['header_size']\n",
        "        self.headers_num = config['headers_num'] # 要不先不实现多头，跑毛爷爷的诗，单头可能也可以了\n",
        "\n",
        "        self.blocks_num = config['blocks_num']\n",
        "        self.ffn_size = config['ffn_size']\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.key = nn.Linear(config.C, config.header_size * config.headers_num)\n",
        "        self.query = nn.Linear(config.C, config.header_size * config.headers_num)\n",
        "        self.value = nn.Linear(config.C, config.header_size * config.headers_num)\n",
        "        self.attn_fc = nn.Linear(config.header_size * config.headers_num, config.C)\n",
        "        # self.feedforward = nn.Linear(config.header, config.ffn_size)\n",
        "        # self.feedforward = nn.Linear(config.header, config.C) # ffn_size应该就是C，不然过下一个block的时候dim和过完embd的不一样了\n",
        "        # 这样应该能学习更多的参数，也不会导致过下一个block的dim不一样\n",
        "        # 但是我没明白的是直接这么提升维度，信息从何而来呢？这就有点像是线性代数里面平行的vector\n",
        "        self.ff1 = nn.Linear(config.C, config.ffn_size)\n",
        "        self.ff2 = nn.Linear(config.ffn_size, config.C)\n",
        "\n",
        "    def split_headers(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, head_dim) and transpose the result.\"\"\"\n",
        "        x = x.view(batch_size, config.T, self.config.headers_num, self.config.header_size)\n",
        "\n",
        "        return x.transpose(1, 2) # shape (B, headers_num, T, header_size) 这里需要thinking一下why\n",
        "\n",
        "    def combine_headers(self, x, batch_size):\n",
        "        \"\"\"\n",
        "            Combine the heads and restore the original last dimension.\n",
        "            x.shape (B, headers_num, T, header_size)\n",
        "        \"\"\"\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        # 这里view中的-1，表示自动计算。其实可以直接用T\n",
        "        return x.view(batch_size, self.config.T, self.config.headers_num * self.config.header_size)\n",
        "\n",
        "    def attn(self, x):\n",
        "        \"\"\"\n",
        "        x.shape (B, T, C)\n",
        "        \"\"\"\n",
        "        config = self.config\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # k, q, v shape (B, headers_num, T, header_size)\n",
        "        k = self.split_headers(self.key(x), B)\n",
        "        q = self.split_headers(self.query(x), B)\n",
        "        v = self.split_headers(self.value(x), B)\n",
        "\n",
        "        tril = torch.tril(torch.ones(config.T, config.T))\n",
        "        wei = q @ k.transpose(-2, -1) # 这个时候用负数的兼容性就体现了\n",
        "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "        wei = wei / (config.header_size ** 0.5)\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        # wei.shape (B, headers_num, T, T)\n",
        "\n",
        "        out = wei @ v # out.shape (B, headers_num, T, header_size)\n",
        "        out = self.combine_headers(out, B)  # out.shape(B, T, config.header_size * config.headers_num)\n",
        "        x = self.attn_fc(out)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def ffn(self, x):\n",
        "        x = self.ff1(x)\n",
        "        x = F.gelu(x, approximate='tanh')\n",
        "        out2 = self.ff2(x)\n",
        "        return out2\n",
        "\n",
        "    def layernorm(self, x, epsilon=1e-5):\n",
        "        mean = torch.mean(x, axis=-1, keepdims=True)\n",
        "        std = torch.sqrt(x.var(axis=-1, keepdims=True) + epsilon)\n",
        "        norm_x = (x - mean) / std\n",
        "        return norm_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 应该还有一个norm，为什么要norm有点忘了，先不实现\n",
        "        x = x + self.attn(self.layernorm(x))\n",
        "        x = x + self.ffn(self.layernorm(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.C)\n",
        "        self.wpe = nn.Embedding(config.T, config.C)\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.blocks_num)])\n",
        "        # why lm_fc. 你可以🤔，每个position都是aggregate左侧的信息，然后再深度计算\n",
        "        # 最后你的task是next token prediction\n",
        "        # 所以你要把这个\"aggregate左侧的信息，然后再深度计算\"的信息，映射到token embd空间\n",
        "        # 然后通过wte逆向 映射出来 tokenid\n",
        "        self.lm_fc = nn.Linear(config.C, config.C)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        token_idx = idx\n",
        "        range_tensor = torch.arange(T)\n",
        "        position_idx = range_tensor.expand(B, T)\n",
        "\n",
        "        x = self.wte(token_idx) + self.wpe(position_idx)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.lm_fc(x) # (B, T, C)\n",
        "        # 使用嵌入层的权重转置进行反向映射\n",
        "        logits = torch.matmul(x, self.wte.weight.t())\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            targets = targets.view(B*T)\n",
        "             # 要softmax吗？没想明白\n",
        "            logits = logits.view(B*T, vocab_size)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "\n",
        "        for step in range(max_new_tokens):\n",
        "            _B, seq_len = idx.shape\n",
        "            if seq_len > self.config.T:\n",
        "                idx_fixedlen = idx[:, -self.config.T:]\n",
        "                _B, seq_len = idx_fixedlen.shape\n",
        "            else:\n",
        "\n",
        "                padding_len = self.config.T - seq_len\n",
        "                idx_fixedlen = F.pad(idx, (0, padding_len), \"constant\", 0)\n",
        "\n",
        "            logits, loss = self(idx_fixedlen)\n",
        "            logits = logits[:, seq_len - 1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "config = GPTConfig({'B': 16, 'T': 32, 'C': 64, 'vocab_size': vocab_size, 'header_size':16, 'headers_num': 4, 'blocks_num': 4, 'ffn_size': 256})\n",
        "m = GPT(config)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# idx = encode('北国风光')\n",
        "# print(idx)\n",
        "\n",
        "batch_size = config.B\n",
        "block_size = config.T\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else valid_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print('token id 0:' + decode([0]))\n",
        "\n",
        "print(f'xb: {xb.shape}')\n",
        "print(f'vocab_size: {vocab_size}')\n",
        "\n",
        "print(f'wpe: {m.wpe}')\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(f'logits: {logits}')\n",
        "print(f'loss: {loss}')\n",
        "\n",
        "gen_idx = torch.tensor([encode('北国风光')], dtype=torch.long)\n",
        "# gen_idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(f'gen_idx: {gen_idx.shape}')\n",
        "print(decode(m.generate(idx = gen_idx, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFDVvZnLNSD7",
        "outputId": "f0cfb3ec-5744-49bf-e548-374761d14fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.339904 M parameters\n",
            "token id 0:\n",
            "\n",
            "xb: torch.Size([16, 32])\n",
            "vocab_size: 2106\n",
            "wpe: Embedding(32, 64)\n",
            "logits: tensor([[ -2.4885,  -3.6816,   0.5207,  ...,  -3.4562,  11.8609,   3.4198],\n",
            "        [ -7.5850,  -4.5639,   3.5216,  ...,  -6.0404,   2.1144,  -7.7048],\n",
            "        [ -0.0517,  -3.1455,  10.8177,  ...,  -6.0670,  11.4114,  -0.5887],\n",
            "        ...,\n",
            "        [  3.6814,   0.2676,  -2.0900,  ...,   6.8973,   1.0004,   4.7384],\n",
            "        [  0.4876,  -2.8634,   8.5212,  ...,  -1.7122,  -0.9736, -15.2228],\n",
            "        [ 13.0663,  -4.5862,   2.1840,  ...,   3.1832,  -0.9255, -13.3510]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "loss: 23.159183502197266\n",
            "gen_idx: torch.Size([1, 4])\n",
            "北国风光判煦比税菩答圃稠统灭李事煦蚩煞些阶钊奠戟莲导静誉胄兼拼饭跌健印峰及伤备蒋加逆乔遵覃晋凡号幽查凡及伤及伤备话停及覃晋凡及岩接加眠株话退凡及伤稍瓯瓜食谢翥加眠株话欲赤交话退凡及伤稍交萋智南裹晋凡及裹亥念食况些雪复仓话杨俏南裹阁否及条秉稍跌愤话退凡及伤及伤备话翔渡凡号加跌愤话退凡复愤庶些魂凡及覃晋凡及伤备话退凡及伤备话退愤萋欺稍交萋号幽查僧话萋嶷狐倭及伤备话奇威凡及伤及伤暗亥晶话遽凡及伤及裹阁否及伤备话最凡复最懿煎跌佗郁凡号幽查懿煎欺接加跌佗旧跌眠株话爱蔑懿煎萋智及陇欺接加跌牢嶷狐晶哭亥复条秉稍眠株蒋加跌眠株话爱哭亥念天换郁哭亥复愤庶话奇跌眠株蒋加跌佗乔答及伤稍潇株话奇实弃话跌眠株话大话奇跌眠株话翔实杀念事嶷雪祥嶷狐备话威及伤备话遽凡号幽查话带眠株话奇威嶷狐倭及陇欺流佗佗郁话翔都样及遇跌佗医卢加眠株话威及伤备话萋嶷拒郁话退凡及伤及伤备话遽凡及伤备话程株庶话奇威及伤及裹阁萋嶷食菽眠株话奇威实杀嶷拒株话奇医凄卢加跌佗烛渡凡号幽查凡及伤备话萋欺医亥及伤稍瓯话程备蒋加跌愤潇株庶些雪株怨程株话威实秉稍交话退凡及陇加跌健印峰及伤稍跌眠株话威及伤备话退凡复眠株话翔愤庶话退愤庶话爱哭亥永凡号加跌退及伤稍骋威\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "VOPpLEioft4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 100\n",
        "max_iter = 5000\n",
        "\n",
        "for iter in range(max_iter):\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iter:\n",
        "        x_val, y_val = get_batch('valid')\n",
        "        logits_val, loss_val = m(x_val, y_val)\n",
        "        print(f'step: {iter}, train loss: {loss}, valid loss:{loss_val}')\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "# 跑了半天loss 3.x，比bigram的还高。多跑几轮能到2.5，loss有波动\n",
        "# 增加header_size，blocks_num， batch_size 没啥用\n",
        "# 还有几个没有优化的:1. 非线性激活 2.LayerNorm 3. MultiHead\n",
        "# 看了下Andrej老师的notebook，step5000 trainloss能降到1.6,valloss能降到1.8\n",
        "# 这里可能有2个原因，1是数据集毛爷爷的诗还是不够多，然后vocab比英语的chars要大不少；2是代码有bug，\n",
        "# 但是还是要想明白一点，预训练和SFT可能不太一样，loss不可能逼近0，因为譬如第一个字去预测第二个字，总是会有loss的，加大block_size，平均下来应该能降低一些loss\n",
        "# 最后通过记录模型参数(太小)发现还是代码有bug，`Block` 实例中的参数，如果不加上nn.ModuleList,不会被 `model.parameters()` 方法捕获，也不会参与优化过程。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wnembm7NmIx",
        "outputId": "59095801-a708-49b6-a431-94235d633523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 23.13743782043457, valid loss:23.249996185302734\n",
            "step: 100, train loss: 6.354671955108643, valid loss:6.173121929168701\n",
            "step: 200, train loss: 5.4620680809021, valid loss:6.11049222946167\n",
            "step: 300, train loss: 5.662128448486328, valid loss:5.6373209953308105\n",
            "step: 400, train loss: 5.1679558753967285, valid loss:5.711599349975586\n",
            "step: 500, train loss: 4.677831649780273, valid loss:5.983814716339111\n",
            "step: 600, train loss: 4.848365783691406, valid loss:5.748012065887451\n",
            "step: 700, train loss: 4.45619535446167, valid loss:6.185412883758545\n",
            "step: 800, train loss: 3.9210309982299805, valid loss:5.808258056640625\n",
            "step: 900, train loss: 4.190140724182129, valid loss:5.753818511962891\n",
            "step: 1000, train loss: 3.8164405822753906, valid loss:5.9853010177612305\n",
            "step: 1100, train loss: 3.2423508167266846, valid loss:6.772550582885742\n",
            "step: 1200, train loss: 3.1564619541168213, valid loss:6.569118499755859\n",
            "step: 1300, train loss: 3.1226515769958496, valid loss:6.061294078826904\n",
            "step: 1400, train loss: 2.6381051540374756, valid loss:6.161025524139404\n",
            "step: 1500, train loss: 2.5783188343048096, valid loss:7.884429931640625\n",
            "step: 1600, train loss: 2.3160958290100098, valid loss:8.014161109924316\n",
            "step: 1700, train loss: 2.1189804077148438, valid loss:7.390343189239502\n",
            "step: 1800, train loss: 1.7116247415542603, valid loss:8.070854187011719\n",
            "step: 1900, train loss: 1.7962510585784912, valid loss:9.888666152954102\n",
            "step: 2000, train loss: 1.414097785949707, valid loss:9.929740905761719\n",
            "step: 2100, train loss: 1.3808047771453857, valid loss:10.137639999389648\n",
            "step: 2200, train loss: 1.1707282066345215, valid loss:10.65943717956543\n",
            "step: 2300, train loss: 1.1631410121917725, valid loss:10.491622924804688\n",
            "step: 2400, train loss: 1.137540578842163, valid loss:11.142854690551758\n",
            "step: 2500, train loss: 0.9988354444503784, valid loss:11.204266548156738\n",
            "step: 2600, train loss: 0.8870669603347778, valid loss:10.474055290222168\n",
            "step: 2700, train loss: 0.8811224102973938, valid loss:11.711979866027832\n",
            "step: 2800, train loss: 0.8444487452507019, valid loss:11.901725769042969\n",
            "step: 2900, train loss: 0.7132939100265503, valid loss:12.545466423034668\n",
            "step: 3000, train loss: 0.7685427069664001, valid loss:11.887646675109863\n",
            "step: 3100, train loss: 0.6091021299362183, valid loss:11.683645248413086\n",
            "step: 3200, train loss: 0.6291784644126892, valid loss:13.354111671447754\n",
            "step: 3300, train loss: 0.6791402101516724, valid loss:12.112665176391602\n",
            "step: 3400, train loss: 0.5918178558349609, valid loss:12.08292007446289\n",
            "step: 3500, train loss: 0.6003862619400024, valid loss:13.070770263671875\n",
            "step: 3600, train loss: 0.5773215293884277, valid loss:12.271685600280762\n",
            "step: 3700, train loss: 0.5417482852935791, valid loss:12.424269676208496\n",
            "step: 3800, train loss: 0.4659838378429413, valid loss:13.433122634887695\n",
            "step: 3900, train loss: 0.4771704375743866, valid loss:12.877375602722168\n",
            "step: 4000, train loss: 0.5623237490653992, valid loss:12.836653709411621\n",
            "step: 4100, train loss: 0.47410497069358826, valid loss:12.762564659118652\n",
            "step: 4200, train loss: 0.5778568387031555, valid loss:13.610174179077148\n",
            "step: 4300, train loss: 0.43670201301574707, valid loss:12.379487037658691\n",
            "step: 4400, train loss: 0.5072713494300842, valid loss:13.158035278320312\n",
            "step: 4500, train loss: 0.43504855036735535, valid loss:12.562936782836914\n",
            "step: 4600, train loss: 0.4744766652584076, valid loss:13.483939170837402\n",
            "step: 4700, train loss: 0.4736781120300293, valid loss:12.849152565002441\n",
            "step: 4800, train loss: 0.48781147599220276, valid loss:13.450811386108398\n",
            "step: 4900, train loss: 0.5128433704376221, valid loss:14.335057258605957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_idx = torch.tensor([encode('红军不怕远征难')], dtype=torch.long)\n",
        "print(decode(m.generate(idx = gen_idx, max_new_tokens=500)[0].tolist())) # 实际效果也一般"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJZssbgYO-Zz",
        "outputId": "deaf0f65-16fd-47a3-cc1c-db0a15d4baff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "红军不怕远征难，万水千山只等闲。\n",
            "\n",
            "五岭逶迤腾细浪，乌蒙磅礴汪野小民夺。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七绝·改诗\n",
            "\n",
            "如许《游风下有叠嶂。\n",
            "\n",
            "三次尊匪须兵怒，遣春，坚固；国形。\n",
            "\n",
            "如鼠技，崇明对鹫置其人地，九社挥白。\n",
            "\n",
            "猖纷磅什赣，桃花烧旧怒，连发卷起农动。\n",
            "\n",
            "旧主干台。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·读报（反苏忆昔闹群蛙）\n",
            "\n",
            "反苏忆昔闹群蛙，今日欣看大反华。\n",
            "\n",
            "回雁血，坚从来重岳。\n",
            "\n",
            "如血。政光减如文是扬。\n",
            "\n",
            "黑此可革命派，公祭分农下谁？一北去。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 长征·答李对难转\n",
            "\n",
            "正乱山染，霜分月。\n",
            "\n",
            "五岭逶抛送起。\n",
            "\n",
            "遗孽世饥，，百万州江堂。\n",
            "猪复不投文年风贺何苛捐枕明赤县。\n",
            "\n",
            "听话准情，不胜利就在明天！\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 五言韵语·军队向前进\n",
            "\n",
            "军队向前进，\n",
            "生万放丈张辉，\n",
            "\n",
            "\n",
            "江山如画堂。\n",
            "\n",
            "以此嫩烟。\n",
            "\n",
            "生鼠起\n",
            "\n",
            "百得竟尘激，我要自毁，例氤起文酣不写就。\n",
            "\n",
            "秋田有疑猜。\n",
            "\n",
            "早苍蝇截，要内苦。\n",
            "\n",
            "对方不须放。\n",
            "\n",
            "念旧药，年军寞披遐买史光，都照如此。\n",
            "\n",
            "\n",
            "\n",
            "# 江人二首·改胡乔木《水龙吟七百细飞寞南春宇进。\n",
            "\n",
            "打倒反革命致天，\n",
            "\n",
            "洞中宋兹残养边，\n",
            "\n",
            "叫一驰努力看。\n",
            "\n",
            "江。\n",
            "为南饮林深千。\n",
            "\n",
            "起冲宋衡草一两由。\n",
            "\n",
            "\n",
            "乡哨所恨，黄洋\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T7Lsk5R91v2y"
      ],
      "authorship_tag": "ABX9TyP4BImPfoLFA6XDK+Fu/HKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}