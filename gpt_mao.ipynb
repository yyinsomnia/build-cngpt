{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyinsomnia/build-cngpt/blob/main/gpt_mao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 环境和依赖\n",
        "\n"
      ],
      "metadata": {
        "id": "3BIWez3s2klu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_fPxK2iaPZD",
        "outputId": "1e1bafec-4b25-4e3c-84ac-196d4f2291c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-14 02:21:53--  https://raw.githubusercontent.com/yyinsomnia/openai-ex/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42761 (42K) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]  41.76K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-14 02:21:54 (1.06 MB/s) - ‘input.txt.1’ saved [42761/42761]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/yyinsomnia/openai-ex/main/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 传统的语言模型bigram"
      ],
      "metadata": {
        "id": "hbR7rffa2zCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "U9iboUIQbZll"
      },
      "outputs": [],
      "source": [
        "with open('input.txt') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4csMaMk4bkxQ",
        "outputId": "8a4c0f4f-6a41-4867-f327-69a822696e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the mao-poetic in characters is : 16498\n"
          ]
        }
      ],
      "source": [
        "print('length of the mao-poetic in characters is :', len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtOUUVQTdEyM",
        "outputId": "aaaf5c20-137c-41b1-f381-94d2090e78c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 西江月·秋收起义\n",
            "\n",
            "军叫工农革命，\n",
            "\n",
            "旗号镰刀斧头。\n",
            "\n",
            "匡庐一带不停留，\n",
            "\n",
            "要向潇湘直进。\n",
            "\n",
            "地主重重压迫，\n",
            "\n",
            "农民个个同仇。\n",
            "\n",
            "秋收时节暮云愁，\n",
            "\n",
            "霹雳一声暴动。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 菩萨蛮·黄鹤楼\n",
            "\n",
            "茫茫九派流中国，\n",
            "\n",
            "沉沉一线穿南北。\n",
            "\n",
            "烟雨莽苍苍，\n",
            "\n",
            "龟蛇锁大江。\n",
            "\n",
            "黄鹤知何去？\n",
            "\n",
            "剩有游人处。\n",
            "\n",
            "把酒酹滔滔，\n",
            "\n",
            "心潮逐浪高！\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·和柳亚子先生\n",
            "\n",
            "饮茶粤海未能忘，索句渝州叶正黄。\n",
            "\n",
            "三十一年还旧国，落花时节读华章。\n",
            "\n",
            "牢骚太盛防肠断，风物长宜放眼量。\n",
            "\n",
            "莫道昆明池水浅，观鱼胜过富春江。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·冬云\n",
            "\n",
            "雪压冬云白絮飞，万花纷谢一时稀。\n",
            "\n",
            "高天滚滚寒流急，大地微微暖气吹。\n",
            "\n",
            "独有英雄驱虎豹，更无豪杰怕熊罴。\n",
            "\n",
            "梅花欢喜漫天雪，冻死苍蝇未足奇。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·到韶山\n",
            "\n",
            "别梦依稀咒逝川，故园三十二年前。\n",
            "\n",
            "红旗卷起农奴戟，黑手高悬霸主鞭。\n",
            "\n",
            "为有牺牲多壮志，敢教日月换新天。\n",
            "\n",
            "喜看稻菽千重浪，遍地英雄下夕烟。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 四言诗·祭母文\n",
            "\n",
            "呜呼吾母，遽然而死。寿五十三，生有七子。\n",
            "\n",
            "七子余三，即东民覃。其他不育，二女二男。\n",
            "\n",
            "育吾兄弟，艰辛备历。摧折作磨，因此遘疾。\n",
            "\n",
            "中间万万，皆伤心史。不忍卒书，待徐温吐。\n",
            "\n",
            "今则欲言，只有两端：一则盛德，一则恨偏。\n",
            "\n",
            "吾母高风，首推博爱。远近亲疏，一皆覆载。\n",
            "\n",
            "恺恻慈祥，感动庶汇。爱力所及，原本真诚。\n",
            "\n",
            "不作诳言，不存欺心。整饬成性，一丝不诡。\n",
            "\n",
            "手泽所经，皆有条理。头脑精密，擘理分情。\n",
            "\n",
            "事无遗算，物无遁形。洁净之风，传遍戚里。\n",
            "\n",
            "不染一尘，身心表里。五德荦荦，乃其大端。\n",
            "\n",
            "合其人格，如在上焉。恨偏所在，三纲之末。\n",
            "\n",
            "有志未伸，有求不获。精神痛苦，以此为卓。\n",
            "\n",
            "天乎人欤，倾地一角。次则儿辈，育之成行。\n",
            "\n",
            "如果未熟，介在青黄。病时揽手，酸心结肠。\n",
            "\n",
            "但呼儿辈，各务为良。又次所怀，好亲至爱。\n",
            "\n",
            "或属素恩，或多劳瘁。大小亲疏，均待报赍。\n",
            "\n",
            "总兹所述，盛德所辉。必秉悃忱，则效不违。\n",
            "\n",
            "致于所恨，必补遗缺。念兹在兹，此心不越。\n",
            "\n",
            "养育深恩，春辉朝霭。报之何时，精禽大海。\n",
            "\n",
            "呜呼吾母，母终未死。躯壳虽隳，灵则万古。\n",
            "\n",
            "有生一日，皆报恩时。有生一日，皆伴亲时。\n",
            "\n",
            "今也言长，时则苦短。惟挈大端，置其粗浅。\n",
            "\n",
            "此时家奠，尽此一觞。后有言陈，与日俱长。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 清平乐 蒋桂战争\n",
            "\n",
            "风云突变，\n",
            "\n",
            "军阀重开战。\n",
            "\n",
            "洒向人\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rVxLO6kDdLl8"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJY9wNQ1ddDZ",
        "outputId": "662d2af3-c922-4290-fc70-11a5e353ae59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " #*AB·“”…　、。《》一丁七万丈三上下不与且世业丛东丝两严丧个中丰临丸丹为主丽举乃久么义之乌乎乐乔乘九也习乡书买乱乾了争事二于云五井亚些亟亡交亥亦产亩京亭亲人亿什仇今介仍从仑仓他仗付仙仞代令以仰仲任份仿伍伏休众会伟传伤伦伫伴伸似但位低住佐何佗余作你佳例供依侠侣侮侯侵便俊俏俘保信修俯俱倒倚倜借倦倭债倾假偏偕做停健偶偷偿傅傍傥催傲僧儿兀元兄充兆先光免兔党入全八公六兮兰共关兴兵其兹养兼冀内冈冉再冒冕写军农冠冢冬冯冰冲决况冷冻净凄准凉凋凌减几凡凤凭凯凰凶出击刀分切划列刘则刚初判利别到制刺前剑剧剩副割剿力劝办功加务劣动努劫劲劳势勇勤勾勿匈匕化北匝匡匪区医十千卅升午半华卑卒卓单卖南博卜占卢卧卫印危即却卵卷卿厂历厉压厚原去县参又及友双反发取受变叛叟叠口古句只叫叭可台史叶号司叹吃各合吉吊同名后吏吐向吓君吟否含听启吴吸吹吻吼吾呀呈告员呜周味呼命和咎咏咒咬咽哀响哎哥哨哪哭唇唐唤唯唱商啥啸啼善喇喊喋喑喜喝嗟嗡嘉嘶嚣囊四回因团园困围固国图囿圃圆土圣在地场均坏坐坑坚坠坤垂垄垒垓埃埋城域培基堂堆堑堕堤堪塔塘塞境墓墙增墟墨墩壁壑壤士壮声壳处备复夏夕外多夜大天太夫央失头夷夸夺奇奈奉奋奏奔奚奠女奴奸她好如妄妆妇妍妖妙妨妻始姐委姿威娄娆娇娓娘娥娴婢媚嫦嫩子孔字存孙季孤学孩孽宁宇守安宋完宏宗官宙定宜宝实宠客宣宫害宴宵家容宽宾宿寂寄密寇富寐寒寞寥寨寰寸对寺寻导寿封射将尊小少尔尘尚尤尧就尸尺尼尽屁层居屈屋展属屠履屦山岁岂岖岛岩岭岳岷岸岿峙峡峥峦峨峰峻崇崎崖崩嵘嶂嶷川州巡工巧巨巫己已巾市布帆师帐帜帝带席帷常幕干平年并幸幽广庄庆床庐应底庙府废度庭庶康廊廓廖延建开异弃弄弓引弟张弥弦弯弱弹强归当形彩彪彭影彻彼往征待徊律徐徒得徘御微德心必忆忍志忘忙忠忡忧快忱念忽怀态怅怎怒怕怜思急性怨怪总恋恍恐恙恨恩息恰恶恺恻悃悚患悬悲情惆惊惜惟惨惯想愁意愚感愤愿慈慌慧慨慷懈懿戈戎戏成我或战戚戟截戴户房所扇手才打托执扫扬扭扶批承技把抒抓投抗折抛报披抱抵抹抽拂拄担拍拒拖拘招拜拥拼拾持挂指挈按挖挟挡挥振挺挽捆捉捐损换捣捧捷掌排掣接推掩插揖握揽搅搏携摄摇摔摧摩撼擘擦攀收改攻放政故效敌救教敝敢散敦数整文斑斗料斜斥斧断斯新斶方施旁旅旋旌族旗无日旦旧旨早旭时旺昂昆昌明昏易昔星映春昨昭是晋晏晓晖晚晨普景晴晶智暂暑暖暗暮暴曙曦曰曲更曹曾最月有朋服朔朗望朝期木未末本朱朵机朽杀杂权李材村杖杜条来杨杭杯杰杷松板极枇枉析枕林果枝枪枯架柏染查柱柳标树株样核格栽桂桃桑桓桥梁梅梓梢梦械检棉棍棒棘棠森棹椒椰楚楫楼概榜槊槐横樯樱橘橙次欢欣欤欧欲欺款歇歌止正此步武歧死殊残殒段毁毅母每毒比毕毛氏民氓气氛氤氲水永汀求汇汉汗汛汝江池污汤汨汪汽沁沆沉沙没沦沧沫河沸治沽沾泄泉泓法波泣泥注泪泱泳泽洁洋洒洗洛洞洢津洪洲活派流浅浆浊测济浓浣浦浩浪浮浴海浸涂消涌涎涔涛涤润涨涯液涸涿淑淘淡深淹添清渊渔渝渡温渭游渺湖湘湾湿溉源溜溟溢溪溯溶滂滋滑滔滚满滨滩滴演漠漫潇潜潭潮潺澄澜激濯瀛火灭灯灰灵灾炉炊炎炬炮炸点炼烂烈烛烟烧烬热烹烽焉焚焰然煎煞煤煦照煮熊熟燃燎燕爆爪爱父爹爽片版牙牛牡牢物牲牺犬状犹狂狐独狮狼猎猖猛猜猪猴猿玉王环现玲珍珠球琅理琉琳琴琼瑜瑟瑶瓒瓜瓦瓯甘甚生用甫田由甲电男画界畔留略番疆疏疑疮疾病痍痕痛瘁瘟登白百的皆皇皋盆盈益监盗盘盛盟目直相盾省眉看真眠眶眺眼着督睿瞻瞽矛矢矣知短石砥破础硬碌碎碑碣碧碰磅磨礴示社祀祖祝神祥票祭祸禁离禽秀秉秋种秕秘租秦积称移秽稀稊程稍税稔稠稻穆穴穷穹空穿突窗窥立竞竟章竦端竹竿笑笔笛笞第笼等答筹算管篆篇簧米类粉粒粗粤粪粮粱精糠素索紧紫累絮繁红纤约级纪纭纲纵纶纷纸线练组绅细织终经结绕绘给绝统继绩续绳维绵绽绿缕编缘缚缨缸缺网罗罢罪置罴羊美羞羡群羽翁翅翔翘翠翥翩翰翻翼老者而耐耕耸耻耿聋联聚聪肃肇肉肝肠肥肯育胄胆背胜胡胥胳胸能脂脑脚腐腰腾膊膏臂自至致舌舍舒舜舞舟航舰舸船艟艨良艰色艾节芒芙花芳苍苏苔苗苛若苦英茂茅茏茜茨茫茶荆草荐荒荔荡荣荦药莫莱莲获莺莽菊菌菜菩菰菽萋营萦萧萨落葱蒋蒙蒲蒸蓉蓝蓬蔑蔚蕡薄薇薜藏虎虏虑虞虫虹虽蚀蚁蚂蚊蚍蚩蛇蛙蛟蛮蜀蜉蜡蜣蜮蝇蝣蝶螳蠢血行衍衔街衡衣补表袋袍袖被裁裂装裔裹西要覃覆见观觉角觞解触言誉誓警计订认讨让训议讯记讲讶许论讼诀证评识诉试诗诚话诡该语误诳说诸读谁调谈谊谋谒谓谖谢谣谪谷豆豚象豪豫豹貌负责贤败账货贫贰贱贵贺贼贾赃资赋赍赏赔赖赚赞赠赣赤赫走赴赶起趁越趋趣足跃跌跖跤路跹踊踏踞踟踪踯蹄蹇蹈蹉蹰蹻躅身躯车转轮轻载辈辉辍输辙辛辞辟辩辰辱边辽达迁迅过迈迎运近返还这进远违连迟迤迫述迴迷迹追退送适逆逊透逐途通逝逞造逢逶逸逼遁遇遍遏遐遒道遗遘遣遥遮遵遽那邦邪邯邻郁郊郎郡部郭郸都鄂酒酣酬酸酹酿醉醒采里重野量金鉴鏖针钊钓钟钢钧钱钺铁铃铐铓铜铢铩铭铲银铺锁锄锋锐锤锦锷镇镜镝镣镰长门问闲间闹闻闽阀阁阅阎阑阒阔阗阜队防阳阴阵阶附际陆陇陈降限陕除险陵陶陷隆隈随隘隳隶难雀雁雄雌雕雨雪雳零雷雹雾需霄霆震霜霞霭露霸霹青静非靠面革鞋鞍鞭韩音韵韶页项顺须顾顿颁颂颅领颇颐频题颜颠风飏飒飘飙飞食餐饕饥饬饭饮饱饷饿馀首香馨马驰驱驻驾骂骄骋骐骗骚骤骥骨骸高鬓鬼魁魂魅魏魔鱮鱼鲁鲂鲋鲜鲲鲸鳖鷃鸟鸡鸣鸦鸿鹏鹤鹫鹰鹿麦黄黍黑鼎鼓鼠齐齿龙龟！（），：；？\n",
            "2106\n"
          ]
        }
      ],
      "source": [
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mdqOWRzPdl0z"
      },
      "outputs": [],
      "source": [
        "stoi = {char:idx for idx, char in enumerate(chars)}\n",
        "itos = {idx: char for idx, char in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abYw3i2heML7",
        "outputId": "d1122805-8cf8-41b2-e815-77847cb376fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "273\n",
            "北\n"
          ]
        }
      ],
      "source": [
        "print(stoi['北'])\n",
        "print(itos[273])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "CyiU6MpTelc3"
      },
      "outputs": [],
      "source": [
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda ids: ''.join([itos[i] for i in ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfIrIOn6fBQu",
        "outputId": "fcb50ee7-43f5-4df5-ad4b-64c706cbe9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[273, 415, 2025, 176]\n",
            "北国风光\n"
          ]
        }
      ],
      "source": [
        "print(encode('北国风光'))\n",
        "print(decode([273, 415, 2025, 176]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmhgBPN7f6xj",
        "outputId": "d0700048-a757-4f8e-d927-26f79d6a5079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16498]) torch.int64\n",
            "tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865, 1772,   47,    0,    0,\n",
            "         203,  330,  628,  204, 2001,  371, 2102,    0,    0,  899,  336, 1930,\n",
            "         231,  887,  479,   12,    0,    0,  275,  657,   15,  642,   23,  158,\n",
            "        1305, 2102,    0,    0, 1679,  349, 1193, 1170, 1335, 1828,   12,    0,\n",
            "           0,  423,   41, 1894, 1894,  308, 1834, 2102,    0,    0,  204, 1067,\n",
            "          34,   34,  344,   85,   12,    0,    0, 1385,  865,  907, 1571,  937,\n",
            "          67,  758, 2102,    0,    0, 1995, 1981,   15,  463,  938,  260,   12,\n",
            "           0,    0,    0,    0,    0,    2,    1, 1609, 1616, 1649,    6, 2089,\n",
            "        2084, 1027,    0,    0, 1589, 1589,   54, 1125, 1126,   35,  415, 2102,\n",
            "           0,    0, 1091, 1091,   15, 1463, 1408,  291,  273,   12,    0,    0,\n",
            "        1220, 1979, 1605, 1576, 1576, 2102,    0,    0, 2098, 1646, 1919,  473,\n",
            "        1082,   12,    0,    0, 2089, 2084, 1354,  123,  311, 2105,    0,    0,\n",
            "         249,  948, 1167,   82,  465,   12,    0,    0,  798, 1884, 1888, 1184,\n",
            "        1184, 2102,    0,    0,  707, 1196, 1846, 1136, 2060, 2099,    0,    0,\n",
            "           0,    0,    0,    2,    1,   17,  699,    6,  372,  996,   70,  521,\n",
            "         175, 1294,    0,    0, 2037, 1590, 1439, 1139,  957, 1542,  712, 2102,\n",
            "        1446,  328, 1163,  626,  335, 1048, 2089,   12,    0,    0,   20,  279,\n",
            "          15,  649, 1826,  903,  415, 2102, 1617, 1574,  907, 1571, 1723,  285,\n",
            "        1415,   12,    0,    0, 1252, 2055,  475, 1332, 1949, 1530,  888, 2102,\n",
            "        2025, 1253, 1931,  542,  868, 1345, 1896,   12,    0,    0, 1600, 1862,\n",
            "         910,  912, 1083, 1073, 1127, 2102, 1683, 2069, 1537, 1820,  561,  918,\n",
            "        1082,   12,    0,    0,    0,    0,    0,    2,    1,   17,  699,    6,\n",
            "         207,   67,    0,    0, 1980,  308,  207,   67, 1320, 1450, 2030, 2102,\n",
            "          18, 1574, 1461, 1732,   15,  907, 1395,   12,    0,    0, 2060,  474,\n",
            "        1185, 1185,  563, 1126,  729, 2102,  473,  423,  705,  705,  935, 1069,\n",
            "         359,   12,    0,    0, 1261,  948, 1583, 1976, 2047, 1633, 1741, 2102,\n",
            "         943,  900, 1739,  977,  726, 1236, 1497,   12,    0,    0, 1010, 1574,\n",
            "        1038,  399, 1192,  474, 1980, 2102,  214, 1053, 1576, 1655,  957, 1777,\n",
            "         483,   12,    0,    0,    0,    0,    0,    2,    1,   17,  699,    6,\n",
            "         243, 2008,  600,    0,    0,  242, 1013,  131, 1395,  375, 1849,  625,\n",
            "        2102,  870,  411,   20,  279,   65,  649,  246,   12,    0,    0, 1452,\n",
            "         899,  303, 1772,  204,  492,  779, 2102, 2091,  786, 2060,  748, 1994,\n",
            "          41, 2004,   12,    0,    0,   40,  948, 1255, 1254,  471,  462,  711,\n",
            "        2102,  876,  874,  901,  947,  839,  890,  474,   12,    0,    0,  399,\n",
            "        1340, 1402, 1611,  280, 1894, 1136, 2102, 1858,  423, 1583, 1976,   22,\n",
            "         469, 1220,   12,    0,    0,    0,    0,    0,    2,    1,  407, 1689,\n",
            "        1713,    6, 1378, 1060,  881,    0,    0,  367,  370,  362, 1060, 2102,\n",
            "        1869, 1229, 1516, 1053,   12,  573,   68,  279,   20, 2102, 1294,  948,\n",
            "          17,  521,   12,    0,    0,   17,  521,  125,   20, 2102,  300,   29,\n",
            "        1067, 1680,   12,  191,   92,   23, 1533, 2102,   65,  491,   65, 1301,\n",
            "          12,    0,    0, 1533,  362,  172,  678, 2102, 1568, 1809,  466,  306,\n",
            "          12,  859,  803,  126, 1368, 2102,  409, 1049, 1864, 1312,   12,    0,\n",
            "           0,   35, 1935,   18,   18, 2102, 1323,  112,  707,  334,   12,   23,\n",
            "         710,  287,   58, 2102,  697,  700, 1165,  348,   12,    0,    0,   86,\n",
            "         237, 1042, 1689, 2102,  329,  948,   31, 1417, 2103,   15,  237, 1332,\n",
            "         706, 2102,   15,  237,  738,  155,   12,    0,    0,  362, 1060, 2060,\n",
            "        2025, 2102, 2042,  847,  292, 1243,   12, 1829, 1824,   81, 1309, 2102,\n",
            "          15, 1323, 1681, 1803,   12,    0,    0,  743,  744,  764, 1376, 2102,\n",
            "         761,  260,  665, 1077,   12, 1243,  253,  784,  315, 2102,  310,  959,\n",
            "        1341, 1714,   12,    0,    0,   23,  126, 1720, 1689, 2102,   23,  524,\n",
            "        1043,  707,   12,  880, 2035,  774,  730, 2102,   15,   30,   23, 1716,\n",
            "          12,    0,    0,  786, 1113,  784, 1470, 2102, 1323,  948,  972, 1280,\n",
            "          12,  479, 1544, 1443,  559, 2102,  862, 1280,  232,  750,   12,    0,\n",
            "           0,   64,  900, 1863, 1429, 2102, 1253,  900, 1856,  688,   12, 1114,\n",
            "         215,   48, 2025, 2102,  111, 1858,  778, 1893,   12,    0,    0,   23,\n",
            "         993,   15,  581, 2102, 1797,  707, 1668, 1893,   12,   68,  706, 1598,\n",
            "        1598, 2102,   44,  191,  473, 1417,   12,    0,    0,  341,  191,   82,\n",
            "        1002, 2102,  496,  422,   21, 1226,   12,  738,  155,  784,  422, 2102,\n",
            "          20, 1458,   48,  958,   12,    0,    0,  948,  711,  957,  116, 2102,\n",
            "         948, 1076,   23, 1603,   12, 1443, 1375, 1316, 1582, 2102,   99, 1049,\n",
            "          40,  288,   12,    0,    0,  474,   50,   82, 1040, 2102,  153,  423,\n",
            "          15, 1685,   12, 1037,  237,  169, 1804, 2102, 1533,   48,  774, 1661,\n",
            "          12,    0,    0,  496,  987,  957, 1237, 2102,   87,  422, 1996, 2089,\n",
            "          12, 1313,  907,  852,  786, 2102, 1887,  707, 1471, 1530,   12,    0,\n",
            "           0,  118,  370,  169, 1804, 2102,  340,  258,   40, 1567,   12,  314,\n",
            "        1037,  784,  721, 2102,  495,   81, 1553, 1243,   12,    0,    0,  776,\n",
            "         596, 1445,  739, 2102,  776,  471,  264, 1317,   12,  473,  578,   81,\n",
            "        1309, 2102,  425,  697,  805, 1759,   12,    0,    0,  733,  192,  784,\n",
            "        1835, 2102, 1332,  706,  784, 1805,   12,  708, 1384,  745,  718, 2102,\n",
            "         237,  871,   23, 1830,   12,    0,    0, 1554,   66,  784,  738, 2102,\n",
            "         708, 1667, 1863, 1491,   12,  719,  192,  422,  192, 2102, 1049,  707,\n",
            "          23, 1774,   12,    0,    0,  193, 1533, 1157,  739, 2102,  918, 1805,\n",
            "         954, 1992,   12,  805,   48,  123,  907, 2102, 1443, 1382,  473, 1139,\n",
            "          12,    0,    0,  367,  370,  362, 1060, 2102, 1060, 1469,  957, 1053,\n",
            "          12, 1798,  464, 1639, 1971, 2102, 1207,  237,   18,  327,   12,    0,\n",
            "           0,  948, 1294,   15,  901, 2102, 1323,  805,  739,  907,   12,  948,\n",
            "        1294,   15,  901, 2102, 1323,  115,   81,  907,   12,    0,    0,   86,\n",
            "          55, 1689, 1931, 2102,  907,  237, 1582, 1355,   12,  754,  826,  473,\n",
            "        1417, 2102, 1496,  191, 1438, 1127,   12,    0,    0, 1049,  907,  552,\n",
            "         490, 2102,  589, 1049,   15, 1686,   12,  346,  948, 1689, 1958, 2102,\n",
            "          24,  901,  145, 1931,   12,    0,    0,    0,    0,    0,    2,    1,\n",
            "        1160,  648,   51,    1, 1619, 1004,  777,   63,    0,    0, 2025,   67,\n",
            "        1409,  322, 2102,    0,    0,  203, 1939, 1894,  672,  777,   12,    0,\n",
            "           0, 1116,  349,   82])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "PgUXhVtUgptd"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "valid_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6581gFPhhJN",
        "outputId": "16b135ca-9449-481f-87b1-4539e55444ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865, 1772])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBHL5NiQhznJ",
        "outputId": "f89ce8b3-d5f2-44dd-e1e4-d2a841c1bd54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when context is tensor([2]), target is 1\n",
            "when context is tensor([2, 1]), target is 1678\n",
            "when context is tensor([   2,    1, 1678]), target is 1082\n",
            "when context is tensor([   2,    1, 1678, 1082]), target is 947\n",
            "when context is tensor([   2,    1, 1678, 1082,  947]), target is 6\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6]), target is 1385\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6, 1385]), target is 865\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865]), target is 1772\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for i in range(block_size):\n",
        "    context = x[:i+1]\n",
        "    target = y[i]\n",
        "    print(f\"when context is {context}, target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3YGZh4vk1bh",
        "outputId": "8f34eb58-c345-468b-8f98-c5aef0f7c50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  17, 1456, 1393,  600,  711, 2102,    0,    0],\n",
            "        [2102,    0,    0, 1678, 2025, 1192,  303,  527],\n",
            "        [ 111, 1858,  778, 1893,   12,    0,    0,   23],\n",
            "        [2042,    6,  866, 1538,   52,  956,   13,  552]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[1456, 1393,  600,  711, 2102,    0,    0,  710],\n",
            "        [   0,    0, 1678, 2025, 1192,  303,  527,  438],\n",
            "        [1858,  778, 1893,   12,    0,    0,   23,  993],\n",
            "        [   6,  866, 1538,   52,  956,   13,  552,   58]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else valid_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# print(get_batch('train'))\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaRE6r7JpaTM",
        "outputId": "f1d71f12-7d82-4476-d723-616bf2a5e072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when context is tensor([17]), target is 1456\n",
            "when context is tensor([  17, 1456]), target is 1393\n",
            "when context is tensor([  17, 1456, 1393]), target is 600\n",
            "when context is tensor([  17, 1456, 1393,  600]), target is 711\n",
            "when context is tensor([  17, 1456, 1393,  600,  711]), target is 2102\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102]), target is 0\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102,    0]), target is 0\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102,    0,    0]), target is 710\n",
            "when context is tensor([2102]), target is 0\n",
            "when context is tensor([2102,    0]), target is 0\n",
            "when context is tensor([2102,    0,    0]), target is 1678\n",
            "when context is tensor([2102,    0,    0, 1678]), target is 2025\n",
            "when context is tensor([2102,    0,    0, 1678, 2025]), target is 1192\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192]), target is 303\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192,  303]), target is 527\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192,  303,  527]), target is 438\n",
            "when context is tensor([111]), target is 1858\n",
            "when context is tensor([ 111, 1858]), target is 778\n",
            "when context is tensor([ 111, 1858,  778]), target is 1893\n",
            "when context is tensor([ 111, 1858,  778, 1893]), target is 12\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12]), target is 0\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0]), target is 0\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0,    0]), target is 23\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0,    0,   23]), target is 993\n",
            "when context is tensor([2042]), target is 6\n",
            "when context is tensor([2042,    6]), target is 866\n",
            "when context is tensor([2042,    6,  866]), target is 1538\n",
            "when context is tensor([2042,    6,  866, 1538]), target is 52\n",
            "when context is tensor([2042,    6,  866, 1538,   52]), target is 956\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956]), target is 13\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956,   13]), target is 552\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956,   13,  552]), target is 58\n"
          ]
        }
      ],
      "source": [
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f'when context is {context}, target is {target}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "cQHyc87uqAFQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # 这里是有讲究的，这个embedding就是通过当前词预测下一个词，这是bigram吗？怎么感觉是unigram\n",
        "        # 没问题，就是bigram。uniqgram就是每个词的出现只与它自己有关，bigram是每个词的出现与它的前一个有关\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4XwzL8uuAKp",
        "outputId": "eed52258-d498-423b-dd7d-646ae99be6dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2106])\n",
            "tensor(7.9996, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "滂东分赞项批孙单蹉泄摔壮聋答方椰勿诉冬苦簧身鲜印育指景素亥莱望租古第奉谣锄卢岖装逝况守辞蒸转解诳误呼料恍误久卷妙辛昌登怕覆执渔因宾问桑忡日见色这镜壮呀鹤雄携入卒酒儿依园妻议必今皆土裹应房薄民跹荣鲁瀛明\n"
          ]
        }
      ],
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "E4Quc7QXnNMl"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xSxhHoROy5f",
        "outputId": "b09a8fba-2337-4795-c972-ba0ee68151ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.122045516967773\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 基本到2.2 就下不去了\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpMJqXjCjU6i",
        "outputId": "680fc1fd-c456-42a3-bee3-0087653d6155"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "torch.zeros(1, 1, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQYMv0aojmNs",
        "outputId": "3e6cc076-02ab-4904-b145-aeb9bb83c1c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 273,  415, 2025,  176]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "gen_idx = torch.tensor([encode('北国风光')], dtype=torch.long)\n",
        "gen_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC3d8-PoO5mj",
        "outputId": "f9be716b-ec26-4409-f87e-7a221f3943d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "北国风光暖物齿燕幕溯扭债图守高卢崩侠商领逊搅霆移中豚螳邪蠢稊蹻天冒淡取唱挽母绅嘶唐炎才挺僧鲂围尔的赴所虑诸鸣臂现盆饿孤遒裹兴蜡孔泓寺啸铩捣焰场郡昏把蚍戴称社浪腰收揽又队球离担瑶汗放粉碌离纲莱绽吹款槐换恨镝秘臂瀛披弓招柏透寞毛山尚谷而险队等妇涤杂冀彼舸五毅椰弃寒巧樯乐两理擘邦湿飙闻网阜跖主走傅觉鲲絮才取…训冈竟了内款迎晚服疏向楼火蜡躅鲸筹遗坑鸣尚菽忍糠拜归突唯弄可镝脚唱雄骥弄级谓百缕由凉利尘舟密险周梦服女后农绽苦菩记踟蓬官蒸宜胡租豪渺琅溶蹻汗刀颁暴暖参抱样绝巾荣现编组昌病貌康真卿实忆引犹柱持单抗海帜熟讶扇苦宁作蹄赖翻修势簧昭生颁约收类宫则碣井满期那颐炮恩新攀堤船想扭（雷衡吏坤媚蹉翥番番戚惆绅宙鏖纪贾本触民愚患划顺征领写侮久洒崇杂司种消塔威层眶牛太茅惨尔牢唤展读惆饷治命拜毁丧树匪尼丸王称骤兆介炉种胳汪樱渺来晶穹颁秀揽呈己洞瓯进启红龙屁竹围晏冀锁友川蚀眶尼迈旋亚嚣续均守懈贤湖当材秉宜岷傥郊退党垒露蒋亦杨佳！逼田氛鲜云摧豫奋在棉页偶枕屋沸奔哀扫摩议城眠星篆版注怨腐该强例仿樱雌扶酿A痍练饬飘凤骗烧翻肃（旧孤哭污经起寒酸庶孙谪织镣肥男失防荆舸已魏房堤叶馨石冬炬数坚如烟托敝踟疑旋浆昆西霆寥毅态灯冉积梓付煮垓\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = gen_idx, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# attention的数学技巧"
      ],
      "metadata": {
        "id": "T7Lsk5R91v2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# 下三角矩阵就能做到和左边的词元attention\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "# 但是这里我们要模拟的不是mask，而是一个K * Q得到的attention权重矩阵，还得归一化\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('---')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('c=')\n",
        "print('---')\n",
        "print(c)\n",
        "\n",
        "# c look as the column combination of a\n",
        "# 2.  0. 0  2\n",
        "# 1.  3. 0. 4\n",
        "# 2/3 2  2. 4.6666\n",
        "\n",
        "# 但是这里应该看成 row combination of b\n",
        "# 1.0 * [2., 7.] + 0.0 * [6., 4.] + 0.0 * [6., 5.] 这就是第1个token的BOW表示\n",
        "# 0.5 * [2., 7.] + 0.5 * [6., 4.] + 0.0 * [6., 5.] 这就是第2个token的BOW表示\n",
        "# 0.3 * [2., 7.] + 0.3 * [6., 4.] + 0.3 * [6., 5.] 这就是第3个token的BOW表示\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCIKXHxu118W",
        "outputId": "9808901d-e08a-4e18-e824-411822c42537"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "---\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO7q8nUBcYAQ",
        "outputId": "d099ea93-8dcd-400b-d701-c6b0ee3f4823"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 先用朴素BOW来表达？\n",
        "# x[b, t] = mean_{i <= t} x[b, i]\n",
        "\n",
        "xbow = torch.zeros(B, T, C)\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        # 私以为比Andrej老师的变量名要好，因为这里也包含了step t这个token\n",
        "        x_t_tprev = x[b, :t+1] # shape (t, C)\n",
        "        xbow[b, t] = torch.mean(x_t_tprev, 0)\n",
        "\n",
        "print(xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuKWNTK8gIf_",
        "outputId": "29f3bc19-38e1-4d2e-f59a-ade9058e7581"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 再用上面那个tril的矩阵乘法来表示\n",
        "# 为什么用tril？因为不用写for循环，并行度更高\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "print('wei_tril:')\n",
        "print(wei)\n",
        "\n",
        "\n",
        "# 这里keepdim=True shape是(8, 1)所以除法就是每行除以对应的那一行的1个sum出来的数\n",
        "# keepdim=False, shape是(8, ), 那就是每一行从左到右每个元素都找到对应的去除\n",
        "wei_sum = wei.sum(1, keepdim=True)\n",
        "\n",
        "print('wei_sum:')\n",
        "print(wei_sum)\n",
        "wei = wei / wei_sum\n",
        "\n",
        "\n",
        "print('wei:')\n",
        "print(wei)\n",
        "\n",
        "xbow2 = wei @ x\n",
        "\n",
        "# 发现太小的值，这里[1,5,1] 是 0.0020，会导致allclose判断为False，\n",
        "# 这个问题很有意思，我记得21年跑这个代码没有这个问题，现在出现了。我也运行了一下Andrej的原始notebook也是Fasle\n",
        "# 说明大概率是python和torch的版本升级导致的，可能是seed rand的数字不一样了\n",
        "print('xbow2:')\n",
        "print(xbow2[1][5][1])\n",
        "print(xbow2[1][5][1] - xbow[1][5][1])\n",
        "torch.allclose(xbow[1][5][1], xbow2[1][5][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K30_1hzjUhb",
        "outputId": "bb77789c-8888-4422-fc59-c7c22528c438"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei_tril:\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei_sum:\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.],\n",
            "        [5.],\n",
            "        [6.],\n",
            "        [7.],\n",
            "        [8.]])\n",
            "wei:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "xbow2:\n",
            "tensor(0.0020)\n",
            "tensor(-3.2363e-08)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 用softmax\n",
        "# 为什么要用softmax？后续如果是模型过完一些层，得出来的是logits而不是probs\n",
        "# logits范围(-inf, inf)，而不是像上面都是类似probs的 1/n 范围[0.0, 1.0]\n",
        "# 过一下softmax，范围就到[0.0, 1.0]了\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = torch.where(wei == 0.0, float('-inf'), wei)\n",
        "\n",
        "\"\"\"\n",
        "在PyTorch中，`F.softmax` 函数用于沿着指定的维度对输入张量进行softmax操作。对于张量 `wei`，其形状为 `(T, T)`，让我们分析一下 `F.softmax(wei, 1)` 和 `F.softmax(wei, -1)` 的含义：\n",
        "\n",
        "1. **`F.softmax(wei, 1)`**:\n",
        "   - 这里的 `1` 表示在第1个维度（即列方向）上进行softmax操作。对于每一行，`F.softmax` 会计算该行内的softmax值。\n",
        "\n",
        "2. **`F.softmax(wei, -1)`**:\n",
        "   - 这里的 `-1` 表示在最后一个维度上进行softmax操作。对于二维张量 `(T, T)` 来说，最后一个维度也是列方向（即维度1）。因此，`F.softmax(wei, -1)` 实际上与 `F.softmax(wei, 1)` 是等价的。\n",
        "\n",
        "总结来说，`F.softmax(wei, 1)` 和 `F.softmax(wei, -1)` 在形状为 `(T, T)` 的张量上执行时，是相同的操作，因为它们都在列方向上计算softmax。\n",
        "\"\"\"\n",
        "wei = F.softmax(wei, 1)\n",
        "print('wei:')\n",
        "print(wei)\n",
        "\n",
        "xbow3 = wei @ x\n",
        "\n",
        "# xbow2和xbow3是一样的\n",
        "# 因为xbow2和xbow3都是先除再加，xbow的mean是先加再除\n",
        "torch.allclose(xbow2, xbow3)"
      ],
      "metadata": {
        "id": "55UHB8xEqJEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d1de87-64a0-4351-c112-781fdee69069"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 终于来了！自注意力self-attention\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 2, 16, 8\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "header_size = 4\n",
        "\n",
        "key = nn.Linear(C, header_size)\n",
        "query = nn.Linear(C, header_size)\n",
        "value = nn.Linear(C, header_size)\n",
        "\n",
        "# k,q,v shape (B, T, head_size)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "v = value(x)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = q @ k.transpose(1, 2) # shape (B, T, T)\n",
        "\n",
        "# 这里wei和tril的shape不一样，看来是能自动boardcast的\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, -1)\n",
        "\n",
        "x_attent_left = wei @ v\n",
        "\n",
        "x_attent_left.shape # shape (B, T, header_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVdYLk_4eDFv",
        "outputId": "458e4fb2-05be-4a5c-dbde-bd55c286a9fd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 16, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意：\n",
        "- 注意力是一种**交流机制**。可以看成是有向图中的节点互相关注，并将所有指向它们的节点的信息加权汇总，权重取决于数据。\n",
        "- 没有空间概念。注意力只是作用于一组向量。这就是我们需要对标记进行位置编码的原因。\n",
        "- 在 \"编码器 \"注意力区块中，只需删除用下三角矩阵进行屏蔽的一行，就可以让所有标记进行交流。这里的区块被称为 \"解码器 \"注意区块，因为它具有下三角形掩码，通常用于自回归设置，如语言建模。\n",
        "- \"自注意力\"只是指键和值与查询的来源相同。在 \"交叉注意力\"中，查询仍由 x 生成，但键和值来自其他外部来源（如编码器模块）。\n",
        "- \"缩放 \"注意力额外将 wei 除以 1/sqrt（head_size）。这样，当输入 Q、K 为单位方差时，wei 也将是单位方差，Softmax 将保持分散，不会过于饱和。说明如下"
      ],
      "metadata": {
        "id": "SSe2mDWXFT5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(8, 4) # shape (T, head_size)\n",
        "print(f'q var: {q.var()}')\n",
        "\n",
        "k = torch.randn(8, 4) # shape (T, head_size)\n",
        "print(f'k var: {k.var()}')\n",
        "\n",
        "wei = q @ k.transpose(0, 1) # shape (T, T)\n",
        "print(f'wei var: {wei.var()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6crEe9MUGsSq",
        "outputId": "1ece6102-8f75-47fc-fc79-1c8f44faf583"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q var: 0.7563359141349792\n",
            "k var: 1.436978816986084\n",
            "wei var: 4.423813819885254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei_norm = wei / (4 ** 1/2) # head_size is 4\n",
        "\n",
        "print(f'wei_norm vars: {wei_norm.var()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny4e6uSrH4IB",
        "outputId": "02408daf-6211-4a73-8621-20debbc00afe"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei_norm vars: 1.1059534549713135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 方差大了过完softmax会变得过于峰值(peak)\n",
        "\n",
        "# 设置打印选项，不使用科学计数法\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "wei_probs = F.softmax(wei, dim=1)\n",
        "print(f'wei_probs: {wei_probs}') # 可以看到[3, 3] 不用科学计数法都是0.0000了，这样整个向量会趋向于one-hot\n",
        "# one-hot有什么不好呢？个人的想法是这里wei是每个token和其他token的关系权重，两两之间总归是有一些关系的\n",
        "\n",
        "wei_norm_probs = F.softmax(wei_norm, dim=1)\n",
        "print(f'wei_norm_probs: {wei_norm_probs}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0bQUBvKIZoS",
        "outputId": "ce8d460f-e7dc-415a-daa0-9ab9ad2367e8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei_probs: tensor([[    0.1156,     0.0277,     0.2300,     0.0235,     0.2698,     0.0331,\n",
            "             0.2560,     0.0443],\n",
            "        [    0.0267,     0.4306,     0.1110,     0.0421,     0.0460,     0.1173,\n",
            "             0.1067,     0.1197],\n",
            "        [    0.7645,     0.0012,     0.0000,     0.0960,     0.0010,     0.1342,\n",
            "             0.0020,     0.0011],\n",
            "        [    0.0358,     0.0988,     0.1712,     0.1134,     0.0943,     0.0105,\n",
            "             0.4289,     0.0470],\n",
            "        [    0.0051,     0.1270,     0.1494,     0.0004,     0.0441,     0.5122,\n",
            "             0.0036,     0.1582],\n",
            "        [    0.0029,     0.0056,     0.0210,     0.0001,     0.0111,     0.0003,\n",
            "             0.9588,     0.0002],\n",
            "        [    0.0037,     0.0291,     0.5980,     0.0740,     0.1340,     0.0015,\n",
            "             0.0699,     0.0898],\n",
            "        [    0.0607,     0.0068,     0.0123,     0.8414,     0.0255,     0.0044,\n",
            "             0.0300,     0.0190]])\n",
            "wei_norm_probs: tensor([[0.1330, 0.0650, 0.1875, 0.0600, 0.2031, 0.0711, 0.1979, 0.0823],\n",
            "        [0.0631, 0.2536, 0.1288, 0.0793, 0.0829, 0.1324, 0.1263, 0.1337],\n",
            "        [0.5143, 0.0206, 0.0034, 0.1823, 0.0185, 0.2155, 0.0264, 0.0192],\n",
            "        [0.0746, 0.1240, 0.1632, 0.1328, 0.1211, 0.0404, 0.2583, 0.0855],\n",
            "        [0.0324, 0.1607, 0.1743, 0.0085, 0.0947, 0.3228, 0.0272, 0.1794],\n",
            "        [0.0386, 0.0535, 0.1036, 0.0053, 0.0752, 0.0131, 0.6997, 0.0109],\n",
            "        [0.0271, 0.0760, 0.3443, 0.1211, 0.1630, 0.0174, 0.1177, 0.1334],\n",
            "        [0.1301, 0.0435, 0.0587, 0.4844, 0.0843, 0.0349, 0.0915, 0.0727]])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3BIWez3s2klu",
        "hbR7rffa2zCf"
      ],
      "authorship_tag": "ABX9TyNr+dQNH37iG9lwVl2LdTez",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}