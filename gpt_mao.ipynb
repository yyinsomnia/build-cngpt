{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyinsomnia/build-cngpt/blob/main/gpt_mao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 环境和依赖\n",
        "\n"
      ],
      "metadata": {
        "id": "3BIWez3s2klu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_fPxK2iaPZD",
        "outputId": "fb453e52-b3a9-4cca-adf6-7bdfe61de5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-13 09:01:49--  https://raw.githubusercontent.com/yyinsomnia/openai-ex/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42761 (42K) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]  41.76K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-07-13 09:01:49 (3.49 MB/s) - ‘input.txt’ saved [42761/42761]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/yyinsomnia/openai-ex/main/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 传统的语言模型bigram"
      ],
      "metadata": {
        "id": "hbR7rffa2zCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9iboUIQbZll"
      },
      "outputs": [],
      "source": [
        "with open('input.txt') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4csMaMk4bkxQ",
        "outputId": "c35f517d-fa6c-4c46-f977-58d2d3e3047a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the mao-poetic in characters is : 16498\n"
          ]
        }
      ],
      "source": [
        "print('length of the mao-poetic in characters is :', len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtOUUVQTdEyM",
        "outputId": "8713cd87-2b8e-44df-a382-3820d31102be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 西江月·秋收起义\n",
            "\n",
            "军叫工农革命，\n",
            "\n",
            "旗号镰刀斧头。\n",
            "\n",
            "匡庐一带不停留，\n",
            "\n",
            "要向潇湘直进。\n",
            "\n",
            "地主重重压迫，\n",
            "\n",
            "农民个个同仇。\n",
            "\n",
            "秋收时节暮云愁，\n",
            "\n",
            "霹雳一声暴动。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 菩萨蛮·黄鹤楼\n",
            "\n",
            "茫茫九派流中国，\n",
            "\n",
            "沉沉一线穿南北。\n",
            "\n",
            "烟雨莽苍苍，\n",
            "\n",
            "龟蛇锁大江。\n",
            "\n",
            "黄鹤知何去？\n",
            "\n",
            "剩有游人处。\n",
            "\n",
            "把酒酹滔滔，\n",
            "\n",
            "心潮逐浪高！\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·和柳亚子先生\n",
            "\n",
            "饮茶粤海未能忘，索句渝州叶正黄。\n",
            "\n",
            "三十一年还旧国，落花时节读华章。\n",
            "\n",
            "牢骚太盛防肠断，风物长宜放眼量。\n",
            "\n",
            "莫道昆明池水浅，观鱼胜过富春江。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·冬云\n",
            "\n",
            "雪压冬云白絮飞，万花纷谢一时稀。\n",
            "\n",
            "高天滚滚寒流急，大地微微暖气吹。\n",
            "\n",
            "独有英雄驱虎豹，更无豪杰怕熊罴。\n",
            "\n",
            "梅花欢喜漫天雪，冻死苍蝇未足奇。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 七律·到韶山\n",
            "\n",
            "别梦依稀咒逝川，故园三十二年前。\n",
            "\n",
            "红旗卷起农奴戟，黑手高悬霸主鞭。\n",
            "\n",
            "为有牺牲多壮志，敢教日月换新天。\n",
            "\n",
            "喜看稻菽千重浪，遍地英雄下夕烟。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 四言诗·祭母文\n",
            "\n",
            "呜呼吾母，遽然而死。寿五十三，生有七子。\n",
            "\n",
            "七子余三，即东民覃。其他不育，二女二男。\n",
            "\n",
            "育吾兄弟，艰辛备历。摧折作磨，因此遘疾。\n",
            "\n",
            "中间万万，皆伤心史。不忍卒书，待徐温吐。\n",
            "\n",
            "今则欲言，只有两端：一则盛德，一则恨偏。\n",
            "\n",
            "吾母高风，首推博爱。远近亲疏，一皆覆载。\n",
            "\n",
            "恺恻慈祥，感动庶汇。爱力所及，原本真诚。\n",
            "\n",
            "不作诳言，不存欺心。整饬成性，一丝不诡。\n",
            "\n",
            "手泽所经，皆有条理。头脑精密，擘理分情。\n",
            "\n",
            "事无遗算，物无遁形。洁净之风，传遍戚里。\n",
            "\n",
            "不染一尘，身心表里。五德荦荦，乃其大端。\n",
            "\n",
            "合其人格，如在上焉。恨偏所在，三纲之末。\n",
            "\n",
            "有志未伸，有求不获。精神痛苦，以此为卓。\n",
            "\n",
            "天乎人欤，倾地一角。次则儿辈，育之成行。\n",
            "\n",
            "如果未熟，介在青黄。病时揽手，酸心结肠。\n",
            "\n",
            "但呼儿辈，各务为良。又次所怀，好亲至爱。\n",
            "\n",
            "或属素恩，或多劳瘁。大小亲疏，均待报赍。\n",
            "\n",
            "总兹所述，盛德所辉。必秉悃忱，则效不违。\n",
            "\n",
            "致于所恨，必补遗缺。念兹在兹，此心不越。\n",
            "\n",
            "养育深恩，春辉朝霭。报之何时，精禽大海。\n",
            "\n",
            "呜呼吾母，母终未死。躯壳虽隳，灵则万古。\n",
            "\n",
            "有生一日，皆报恩时。有生一日，皆伴亲时。\n",
            "\n",
            "今也言长，时则苦短。惟挈大端，置其粗浅。\n",
            "\n",
            "此时家奠，尽此一觞。后有言陈，与日俱长。\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# 清平乐 蒋桂战争\n",
            "\n",
            "风云突变，\n",
            "\n",
            "军阀重开战。\n",
            "\n",
            "洒向人\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVxLO6kDdLl8"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJY9wNQ1ddDZ",
        "outputId": "b1ecdce5-7afb-41b6-ca89-7e93af46eb82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " #*AB·“”…　、。《》一丁七万丈三上下不与且世业丛东丝两严丧个中丰临丸丹为主丽举乃久么义之乌乎乐乔乘九也习乡书买乱乾了争事二于云五井亚些亟亡交亥亦产亩京亭亲人亿什仇今介仍从仑仓他仗付仙仞代令以仰仲任份仿伍伏休众会伟传伤伦伫伴伸似但位低住佐何佗余作你佳例供依侠侣侮侯侵便俊俏俘保信修俯俱倒倚倜借倦倭债倾假偏偕做停健偶偷偿傅傍傥催傲僧儿兀元兄充兆先光免兔党入全八公六兮兰共关兴兵其兹养兼冀内冈冉再冒冕写军农冠冢冬冯冰冲决况冷冻净凄准凉凋凌减几凡凤凭凯凰凶出击刀分切划列刘则刚初判利别到制刺前剑剧剩副割剿力劝办功加务劣动努劫劲劳势勇勤勾勿匈匕化北匝匡匪区医十千卅升午半华卑卒卓单卖南博卜占卢卧卫印危即却卵卷卿厂历厉压厚原去县参又及友双反发取受变叛叟叠口古句只叫叭可台史叶号司叹吃各合吉吊同名后吏吐向吓君吟否含听启吴吸吹吻吼吾呀呈告员呜周味呼命和咎咏咒咬咽哀响哎哥哨哪哭唇唐唤唯唱商啥啸啼善喇喊喋喑喜喝嗟嗡嘉嘶嚣囊四回因团园困围固国图囿圃圆土圣在地场均坏坐坑坚坠坤垂垄垒垓埃埋城域培基堂堆堑堕堤堪塔塘塞境墓墙增墟墨墩壁壑壤士壮声壳处备复夏夕外多夜大天太夫央失头夷夸夺奇奈奉奋奏奔奚奠女奴奸她好如妄妆妇妍妖妙妨妻始姐委姿威娄娆娇娓娘娥娴婢媚嫦嫩子孔字存孙季孤学孩孽宁宇守安宋完宏宗官宙定宜宝实宠客宣宫害宴宵家容宽宾宿寂寄密寇富寐寒寞寥寨寰寸对寺寻导寿封射将尊小少尔尘尚尤尧就尸尺尼尽屁层居屈屋展属屠履屦山岁岂岖岛岩岭岳岷岸岿峙峡峥峦峨峰峻崇崎崖崩嵘嶂嶷川州巡工巧巨巫己已巾市布帆师帐帜帝带席帷常幕干平年并幸幽广庄庆床庐应底庙府废度庭庶康廊廓廖延建开异弃弄弓引弟张弥弦弯弱弹强归当形彩彪彭影彻彼往征待徊律徐徒得徘御微德心必忆忍志忘忙忠忡忧快忱念忽怀态怅怎怒怕怜思急性怨怪总恋恍恐恙恨恩息恰恶恺恻悃悚患悬悲情惆惊惜惟惨惯想愁意愚感愤愿慈慌慧慨慷懈懿戈戎戏成我或战戚戟截戴户房所扇手才打托执扫扬扭扶批承技把抒抓投抗折抛报披抱抵抹抽拂拄担拍拒拖拘招拜拥拼拾持挂指挈按挖挟挡挥振挺挽捆捉捐损换捣捧捷掌排掣接推掩插揖握揽搅搏携摄摇摔摧摩撼擘擦攀收改攻放政故效敌救教敝敢散敦数整文斑斗料斜斥斧断斯新斶方施旁旅旋旌族旗无日旦旧旨早旭时旺昂昆昌明昏易昔星映春昨昭是晋晏晓晖晚晨普景晴晶智暂暑暖暗暮暴曙曦曰曲更曹曾最月有朋服朔朗望朝期木未末本朱朵机朽杀杂权李材村杖杜条来杨杭杯杰杷松板极枇枉析枕林果枝枪枯架柏染查柱柳标树株样核格栽桂桃桑桓桥梁梅梓梢梦械检棉棍棒棘棠森棹椒椰楚楫楼概榜槊槐横樯樱橘橙次欢欣欤欧欲欺款歇歌止正此步武歧死殊残殒段毁毅母每毒比毕毛氏民氓气氛氤氲水永汀求汇汉汗汛汝江池污汤汨汪汽沁沆沉沙没沦沧沫河沸治沽沾泄泉泓法波泣泥注泪泱泳泽洁洋洒洗洛洞洢津洪洲活派流浅浆浊测济浓浣浦浩浪浮浴海浸涂消涌涎涔涛涤润涨涯液涸涿淑淘淡深淹添清渊渔渝渡温渭游渺湖湘湾湿溉源溜溟溢溪溯溶滂滋滑滔滚满滨滩滴演漠漫潇潜潭潮潺澄澜激濯瀛火灭灯灰灵灾炉炊炎炬炮炸点炼烂烈烛烟烧烬热烹烽焉焚焰然煎煞煤煦照煮熊熟燃燎燕爆爪爱父爹爽片版牙牛牡牢物牲牺犬状犹狂狐独狮狼猎猖猛猜猪猴猿玉王环现玲珍珠球琅理琉琳琴琼瑜瑟瑶瓒瓜瓦瓯甘甚生用甫田由甲电男画界畔留略番疆疏疑疮疾病痍痕痛瘁瘟登白百的皆皇皋盆盈益监盗盘盛盟目直相盾省眉看真眠眶眺眼着督睿瞻瞽矛矢矣知短石砥破础硬碌碎碑碣碧碰磅磨礴示社祀祖祝神祥票祭祸禁离禽秀秉秋种秕秘租秦积称移秽稀稊程稍税稔稠稻穆穴穷穹空穿突窗窥立竞竟章竦端竹竿笑笔笛笞第笼等答筹算管篆篇簧米类粉粒粗粤粪粮粱精糠素索紧紫累絮繁红纤约级纪纭纲纵纶纷纸线练组绅细织终经结绕绘给绝统继绩续绳维绵绽绿缕编缘缚缨缸缺网罗罢罪置罴羊美羞羡群羽翁翅翔翘翠翥翩翰翻翼老者而耐耕耸耻耿聋联聚聪肃肇肉肝肠肥肯育胄胆背胜胡胥胳胸能脂脑脚腐腰腾膊膏臂自至致舌舍舒舜舞舟航舰舸船艟艨良艰色艾节芒芙花芳苍苏苔苗苛若苦英茂茅茏茜茨茫茶荆草荐荒荔荡荣荦药莫莱莲获莺莽菊菌菜菩菰菽萋营萦萧萨落葱蒋蒙蒲蒸蓉蓝蓬蔑蔚蕡薄薇薜藏虎虏虑虞虫虹虽蚀蚁蚂蚊蚍蚩蛇蛙蛟蛮蜀蜉蜡蜣蜮蝇蝣蝶螳蠢血行衍衔街衡衣补表袋袍袖被裁裂装裔裹西要覃覆见观觉角觞解触言誉誓警计订认讨让训议讯记讲讶许论讼诀证评识诉试诗诚话诡该语误诳说诸读谁调谈谊谋谒谓谖谢谣谪谷豆豚象豪豫豹貌负责贤败账货贫贰贱贵贺贼贾赃资赋赍赏赔赖赚赞赠赣赤赫走赴赶起趁越趋趣足跃跌跖跤路跹踊踏踞踟踪踯蹄蹇蹈蹉蹰蹻躅身躯车转轮轻载辈辉辍输辙辛辞辟辩辰辱边辽达迁迅过迈迎运近返还这进远违连迟迤迫述迴迷迹追退送适逆逊透逐途通逝逞造逢逶逸逼遁遇遍遏遐遒道遗遘遣遥遮遵遽那邦邪邯邻郁郊郎郡部郭郸都鄂酒酣酬酸酹酿醉醒采里重野量金鉴鏖针钊钓钟钢钧钱钺铁铃铐铓铜铢铩铭铲银铺锁锄锋锐锤锦锷镇镜镝镣镰长门问闲间闹闻闽阀阁阅阎阑阒阔阗阜队防阳阴阵阶附际陆陇陈降限陕除险陵陶陷隆隈随隘隳隶难雀雁雄雌雕雨雪雳零雷雹雾需霄霆震霜霞霭露霸霹青静非靠面革鞋鞍鞭韩音韵韶页项顺须顾顿颁颂颅领颇颐频题颜颠风飏飒飘飙飞食餐饕饥饬饭饮饱饷饿馀首香馨马驰驱驻驾骂骄骋骐骗骚骤骥骨骸高鬓鬼魁魂魅魏魔鱮鱼鲁鲂鲋鲜鲲鲸鳖鷃鸟鸡鸣鸦鸿鹏鹤鹫鹰鹿麦黄黍黑鼎鼓鼠齐齿龙龟！（），：；？\n",
            "2106\n"
          ]
        }
      ],
      "source": [
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdqOWRzPdl0z"
      },
      "outputs": [],
      "source": [
        "stoi = {char:idx for idx, char in enumerate(chars)}\n",
        "itos = {idx: char for idx, char in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abYw3i2heML7",
        "outputId": "a20d7ff5-c16c-4789-8863-e825199ed139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "273\n",
            "北\n"
          ]
        }
      ],
      "source": [
        "print(stoi['北'])\n",
        "print(itos[273])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyiU6MpTelc3"
      },
      "outputs": [],
      "source": [
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda ids: ''.join([itos[i] for i in ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfIrIOn6fBQu",
        "outputId": "54e4e4fd-b03d-4207-ac3d-07b934dd28d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[273, 415, 2025, 176]\n",
            "北国风光\n"
          ]
        }
      ],
      "source": [
        "print(encode('北国风光'))\n",
        "print(decode([273, 415, 2025, 176]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmhgBPN7f6xj",
        "outputId": "f546878a-9b38-469e-e6ae-3a0ef9fdb198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16498]) torch.int64\n",
            "tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865, 1772,   47,    0,    0,\n",
            "         203,  330,  628,  204, 2001,  371, 2102,    0,    0,  899,  336, 1930,\n",
            "         231,  887,  479,   12,    0,    0,  275,  657,   15,  642,   23,  158,\n",
            "        1305, 2102,    0,    0, 1679,  349, 1193, 1170, 1335, 1828,   12,    0,\n",
            "           0,  423,   41, 1894, 1894,  308, 1834, 2102,    0,    0,  204, 1067,\n",
            "          34,   34,  344,   85,   12,    0,    0, 1385,  865,  907, 1571,  937,\n",
            "          67,  758, 2102,    0,    0, 1995, 1981,   15,  463,  938,  260,   12,\n",
            "           0,    0,    0,    0,    0,    2,    1, 1609, 1616, 1649,    6, 2089,\n",
            "        2084, 1027,    0,    0, 1589, 1589,   54, 1125, 1126,   35,  415, 2102,\n",
            "           0,    0, 1091, 1091,   15, 1463, 1408,  291,  273,   12,    0,    0,\n",
            "        1220, 1979, 1605, 1576, 1576, 2102,    0,    0, 2098, 1646, 1919,  473,\n",
            "        1082,   12,    0,    0, 2089, 2084, 1354,  123,  311, 2105,    0,    0,\n",
            "         249,  948, 1167,   82,  465,   12,    0,    0,  798, 1884, 1888, 1184,\n",
            "        1184, 2102,    0,    0,  707, 1196, 1846, 1136, 2060, 2099,    0,    0,\n",
            "           0,    0,    0,    2,    1,   17,  699,    6,  372,  996,   70,  521,\n",
            "         175, 1294,    0,    0, 2037, 1590, 1439, 1139,  957, 1542,  712, 2102,\n",
            "        1446,  328, 1163,  626,  335, 1048, 2089,   12,    0,    0,   20,  279,\n",
            "          15,  649, 1826,  903,  415, 2102, 1617, 1574,  907, 1571, 1723,  285,\n",
            "        1415,   12,    0,    0, 1252, 2055,  475, 1332, 1949, 1530,  888, 2102,\n",
            "        2025, 1253, 1931,  542,  868, 1345, 1896,   12,    0,    0, 1600, 1862,\n",
            "         910,  912, 1083, 1073, 1127, 2102, 1683, 2069, 1537, 1820,  561,  918,\n",
            "        1082,   12,    0,    0,    0,    0,    0,    2,    1,   17,  699,    6,\n",
            "         207,   67,    0,    0, 1980,  308,  207,   67, 1320, 1450, 2030, 2102,\n",
            "          18, 1574, 1461, 1732,   15,  907, 1395,   12,    0,    0, 2060,  474,\n",
            "        1185, 1185,  563, 1126,  729, 2102,  473,  423,  705,  705,  935, 1069,\n",
            "         359,   12,    0,    0, 1261,  948, 1583, 1976, 2047, 1633, 1741, 2102,\n",
            "         943,  900, 1739,  977,  726, 1236, 1497,   12,    0,    0, 1010, 1574,\n",
            "        1038,  399, 1192,  474, 1980, 2102,  214, 1053, 1576, 1655,  957, 1777,\n",
            "         483,   12,    0,    0,    0,    0,    0,    2,    1,   17,  699,    6,\n",
            "         243, 2008,  600,    0,    0,  242, 1013,  131, 1395,  375, 1849,  625,\n",
            "        2102,  870,  411,   20,  279,   65,  649,  246,   12,    0,    0, 1452,\n",
            "         899,  303, 1772,  204,  492,  779, 2102, 2091,  786, 2060,  748, 1994,\n",
            "          41, 2004,   12,    0,    0,   40,  948, 1255, 1254,  471,  462,  711,\n",
            "        2102,  876,  874,  901,  947,  839,  890,  474,   12,    0,    0,  399,\n",
            "        1340, 1402, 1611,  280, 1894, 1136, 2102, 1858,  423, 1583, 1976,   22,\n",
            "         469, 1220,   12,    0,    0,    0,    0,    0,    2,    1,  407, 1689,\n",
            "        1713,    6, 1378, 1060,  881,    0,    0,  367,  370,  362, 1060, 2102,\n",
            "        1869, 1229, 1516, 1053,   12,  573,   68,  279,   20, 2102, 1294,  948,\n",
            "          17,  521,   12,    0,    0,   17,  521,  125,   20, 2102,  300,   29,\n",
            "        1067, 1680,   12,  191,   92,   23, 1533, 2102,   65,  491,   65, 1301,\n",
            "          12,    0,    0, 1533,  362,  172,  678, 2102, 1568, 1809,  466,  306,\n",
            "          12,  859,  803,  126, 1368, 2102,  409, 1049, 1864, 1312,   12,    0,\n",
            "           0,   35, 1935,   18,   18, 2102, 1323,  112,  707,  334,   12,   23,\n",
            "         710,  287,   58, 2102,  697,  700, 1165,  348,   12,    0,    0,   86,\n",
            "         237, 1042, 1689, 2102,  329,  948,   31, 1417, 2103,   15,  237, 1332,\n",
            "         706, 2102,   15,  237,  738,  155,   12,    0,    0,  362, 1060, 2060,\n",
            "        2025, 2102, 2042,  847,  292, 1243,   12, 1829, 1824,   81, 1309, 2102,\n",
            "          15, 1323, 1681, 1803,   12,    0,    0,  743,  744,  764, 1376, 2102,\n",
            "         761,  260,  665, 1077,   12, 1243,  253,  784,  315, 2102,  310,  959,\n",
            "        1341, 1714,   12,    0,    0,   23,  126, 1720, 1689, 2102,   23,  524,\n",
            "        1043,  707,   12,  880, 2035,  774,  730, 2102,   15,   30,   23, 1716,\n",
            "          12,    0,    0,  786, 1113,  784, 1470, 2102, 1323,  948,  972, 1280,\n",
            "          12,  479, 1544, 1443,  559, 2102,  862, 1280,  232,  750,   12,    0,\n",
            "           0,   64,  900, 1863, 1429, 2102, 1253,  900, 1856,  688,   12, 1114,\n",
            "         215,   48, 2025, 2102,  111, 1858,  778, 1893,   12,    0,    0,   23,\n",
            "         993,   15,  581, 2102, 1797,  707, 1668, 1893,   12,   68,  706, 1598,\n",
            "        1598, 2102,   44,  191,  473, 1417,   12,    0,    0,  341,  191,   82,\n",
            "        1002, 2102,  496,  422,   21, 1226,   12,  738,  155,  784,  422, 2102,\n",
            "          20, 1458,   48,  958,   12,    0,    0,  948,  711,  957,  116, 2102,\n",
            "         948, 1076,   23, 1603,   12, 1443, 1375, 1316, 1582, 2102,   99, 1049,\n",
            "          40,  288,   12,    0,    0,  474,   50,   82, 1040, 2102,  153,  423,\n",
            "          15, 1685,   12, 1037,  237,  169, 1804, 2102, 1533,   48,  774, 1661,\n",
            "          12,    0,    0,  496,  987,  957, 1237, 2102,   87,  422, 1996, 2089,\n",
            "          12, 1313,  907,  852,  786, 2102, 1887,  707, 1471, 1530,   12,    0,\n",
            "           0,  118,  370,  169, 1804, 2102,  340,  258,   40, 1567,   12,  314,\n",
            "        1037,  784,  721, 2102,  495,   81, 1553, 1243,   12,    0,    0,  776,\n",
            "         596, 1445,  739, 2102,  776,  471,  264, 1317,   12,  473,  578,   81,\n",
            "        1309, 2102,  425,  697,  805, 1759,   12,    0,    0,  733,  192,  784,\n",
            "        1835, 2102, 1332,  706,  784, 1805,   12,  708, 1384,  745,  718, 2102,\n",
            "         237,  871,   23, 1830,   12,    0,    0, 1554,   66,  784,  738, 2102,\n",
            "         708, 1667, 1863, 1491,   12,  719,  192,  422,  192, 2102, 1049,  707,\n",
            "          23, 1774,   12,    0,    0,  193, 1533, 1157,  739, 2102,  918, 1805,\n",
            "         954, 1992,   12,  805,   48,  123,  907, 2102, 1443, 1382,  473, 1139,\n",
            "          12,    0,    0,  367,  370,  362, 1060, 2102, 1060, 1469,  957, 1053,\n",
            "          12, 1798,  464, 1639, 1971, 2102, 1207,  237,   18,  327,   12,    0,\n",
            "           0,  948, 1294,   15,  901, 2102, 1323,  805,  739,  907,   12,  948,\n",
            "        1294,   15,  901, 2102, 1323,  115,   81,  907,   12,    0,    0,   86,\n",
            "          55, 1689, 1931, 2102,  907,  237, 1582, 1355,   12,  754,  826,  473,\n",
            "        1417, 2102, 1496,  191, 1438, 1127,   12,    0,    0, 1049,  907,  552,\n",
            "         490, 2102,  589, 1049,   15, 1686,   12,  346,  948, 1689, 1958, 2102,\n",
            "          24,  901,  145, 1931,   12,    0,    0,    0,    0,    0,    2,    1,\n",
            "        1160,  648,   51,    1, 1619, 1004,  777,   63,    0,    0, 2025,   67,\n",
            "        1409,  322, 2102,    0,    0,  203, 1939, 1894,  672,  777,   12,    0,\n",
            "           0, 1116,  349,   82])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgUXhVtUgptd"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "valid_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6581gFPhhJN",
        "outputId": "89a75ac8-53d5-424f-ef99-22503181f880"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865, 1772])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBHL5NiQhznJ",
        "outputId": "90b1b47c-dff5-4d7e-8abe-c2d31920c0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when context is tensor([2]), target is 1\n",
            "when context is tensor([2, 1]), target is 1678\n",
            "when context is tensor([   2,    1, 1678]), target is 1082\n",
            "when context is tensor([   2,    1, 1678, 1082]), target is 947\n",
            "when context is tensor([   2,    1, 1678, 1082,  947]), target is 6\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6]), target is 1385\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6, 1385]), target is 865\n",
            "when context is tensor([   2,    1, 1678, 1082,  947,    6, 1385,  865]), target is 1772\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for i in range(block_size):\n",
        "    context = x[:i+1]\n",
        "    target = y[i]\n",
        "    print(f\"when context is {context}, target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3YGZh4vk1bh",
        "outputId": "f4f7f30f-3fd3-4c53-f6d1-d57490936ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  17, 1456, 1393,  600,  711, 2102,    0,    0],\n",
            "        [2102,    0,    0, 1678, 2025, 1192,  303,  527],\n",
            "        [ 111, 1858,  778, 1893,   12,    0,    0,   23],\n",
            "        [2042,    6,  866, 1538,   52,  956,   13,  552]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[1456, 1393,  600,  711, 2102,    0,    0,  710],\n",
            "        [   0,    0, 1678, 2025, 1192,  303,  527,  438],\n",
            "        [1858,  778, 1893,   12,    0,    0,   23,  993],\n",
            "        [   6,  866, 1538,   52,  956,   13,  552,   58]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else valid_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# print(get_batch('train'))\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaRE6r7JpaTM",
        "outputId": "1c0222f8-ac58-4a83-e925-db896fc59c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when context is tensor([17]), target is 1456\n",
            "when context is tensor([  17, 1456]), target is 1393\n",
            "when context is tensor([  17, 1456, 1393]), target is 600\n",
            "when context is tensor([  17, 1456, 1393,  600]), target is 711\n",
            "when context is tensor([  17, 1456, 1393,  600,  711]), target is 2102\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102]), target is 0\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102,    0]), target is 0\n",
            "when context is tensor([  17, 1456, 1393,  600,  711, 2102,    0,    0]), target is 710\n",
            "when context is tensor([2102]), target is 0\n",
            "when context is tensor([2102,    0]), target is 0\n",
            "when context is tensor([2102,    0,    0]), target is 1678\n",
            "when context is tensor([2102,    0,    0, 1678]), target is 2025\n",
            "when context is tensor([2102,    0,    0, 1678, 2025]), target is 1192\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192]), target is 303\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192,  303]), target is 527\n",
            "when context is tensor([2102,    0,    0, 1678, 2025, 1192,  303,  527]), target is 438\n",
            "when context is tensor([111]), target is 1858\n",
            "when context is tensor([ 111, 1858]), target is 778\n",
            "when context is tensor([ 111, 1858,  778]), target is 1893\n",
            "when context is tensor([ 111, 1858,  778, 1893]), target is 12\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12]), target is 0\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0]), target is 0\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0,    0]), target is 23\n",
            "when context is tensor([ 111, 1858,  778, 1893,   12,    0,    0,   23]), target is 993\n",
            "when context is tensor([2042]), target is 6\n",
            "when context is tensor([2042,    6]), target is 866\n",
            "when context is tensor([2042,    6,  866]), target is 1538\n",
            "when context is tensor([2042,    6,  866, 1538]), target is 52\n",
            "when context is tensor([2042,    6,  866, 1538,   52]), target is 956\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956]), target is 13\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956,   13]), target is 552\n",
            "when context is tensor([2042,    6,  866, 1538,   52,  956,   13,  552]), target is 58\n"
          ]
        }
      ],
      "source": [
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f'when context is {context}, target is {target}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQHyc87uqAFQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # 这里是有讲究的，这个embedding就是通过当前词预测下一个词，这是bigram吗？怎么感觉是unigram\n",
        "        # 没问题，就是bigram。uniqgram就是每个词的出现只与它自己有关，bigram是每个词的出现与它的前一个有关\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4XwzL8uuAKp",
        "outputId": "10c78106-8c1c-4438-8689-d69cefb69cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2106])\n",
            "tensor(7.9996, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "滂东分赞项批孙单蹉泄摔壮聋答方椰勿诉冬苦簧身鲜印育指景素亥莱望租古第奉谣锄卢岖装逝况守辞蒸转解诳误呼料恍误久卷妙辛昌登怕覆执渔因宾问桑忡日见色这镜壮呀鹤雄携入卒酒儿依园妻议必今皆土裹应房薄民跹荣鲁瀛明\n"
          ]
        }
      ],
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Quc7QXnNMl"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xSxhHoROy5f",
        "outputId": "69f65b73-3928-4dad-eb1e-d1b5da836243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9428210258483887\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 基本到2.2 就下不去了\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpMJqXjCjU6i",
        "outputId": "3af03021-adfc-454a-cb8b-78918588256b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "torch.zeros(1, 1, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQYMv0aojmNs",
        "outputId": "52bebf60-1c4d-4434-b396-fbfe5345850d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 273,  415, 2025,  176]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "gen_idx = torch.tensor([encode('北国风光')], dtype=torch.long)\n",
        "gen_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC3d8-PoO5mj",
        "outputId": "585bef77-d154-4082-c568-4076961eed07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "北国风光。\n",
            "\n",
            "致尽红旗过薄浣楼狼除阁检效赍村证张传蜮妍结肇脚泪颁逐寰这遏杨导故辞节两疾贱艨巡击楫矛侯节偏瑜间颅仍停肯讶旋风追热县喋语·改康荣朽添亿瞻寄散蛇郸虏蚂救会惟　字娓罢粗固倒军纵入诗句迤铐猜严木镇炬有两囿枇斶驻羊旭艰屠喇间，如虎堤氤蛙牛南陷岳冯绩毕恋尔可该俯赃表寰寿酸冢溟毅肉虏涸尔钢赔鄂偿插荣意多文咏汨蕡银闲。\n",
            "\n",
            "求怜去？\n",
            "\n",
            "\n",
            "旧弄叛贫樯借颇工农民满橘娓桃仇。\n",
            "横判荆萋圆报袍颇哀赔奠豪，\n",
            "一生乃忘蜡置讨置草麦银膊嘉渺帆幸凤饿领谒兆贼。\n",
            "\n",
            "六洲赔虏卅买欧塞按奉恺敦鸿尚主红蹇炊欧母终兄夷豚兔怅煎欤分裂足题掩多抓倦托毅卿即卑宁观摔漫春夏兮滚汀直料升伟业见柱通府妻遇窗虎菩磨，把寞治艟洛萧生涯顿嘉眼沦刚\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "吾呀杯鬓寻位坚。\n",
            "# 七律·改陈眺孤阵辉影相竞徐章西直穿丹亩票际衣被豹捧凯务夕铃湘鼎泽信绅通取孽海翻奉尤迫。\n",
            "\n",
            "到演妙雹恋岖列友面点涎帐富饿懿陇踪起眉参莽减刚贰遗帷橘铭城东汉黍床棒述鲜隆猜似械寨椒溯偷盾民宗卑天高富秽梁感鹰残骸鸡羽陷联盾猴现载炊氤荔穷寇冕判等尼翘欢各魂祖蓝宽把薄亩载嵘郊稍踊铺笞列撼硬船灯票廖懿栽球倒，\n",
            "开颜斶笼母棹映柱开争解逐紫鞍装傍战懈名青山红霞两况赖赫其遏慈饱嶂崎踯指\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = gen_idx, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# attention的数学技巧"
      ],
      "metadata": {
        "id": "T7Lsk5R91v2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# 下三角矩阵就能做到和左边的词元attention\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "# 但是这里我们要模拟的不是mask，而是一个K * Q得到的attention权重矩阵，还得归一化\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('---')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('c=')\n",
        "print('---')\n",
        "print(c)\n",
        "\n",
        "# c look as the column combination of a\n",
        "# 2.  0. 0  2\n",
        "# 1.  3. 0. 4\n",
        "# 2/3 2  2. 4.6666\n",
        "\n",
        "# 但是这里应该看成 row combination of b\n",
        "# 1.0 * [2., 7.] + 0.0 * [6., 4.] + 0.0 * [6., 5.] 这就是第1个token的BOW表示\n",
        "# 0.5 * [2., 7.] + 0.5 * [6., 4.] + 0.0 * [6., 5.] 这就是第2个token的BOW表示\n",
        "# 0.3 * [2., 7.] + 0.3 * [6., 4.] + 0.3 * [6., 5.] 这就是第3个token的BOW表示\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCIKXHxu118W",
        "outputId": "27dd54bb-70f6-4b5a-e6de-c43bca0c9112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "---\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO7q8nUBcYAQ",
        "outputId": "c1c8c294-c2ad-45df-9417-6b4c16f65048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 先用朴素BOW来表达？\n",
        "# x[b, t] = mean_{i <= t} x[b, i]\n",
        "\n",
        "xbow = torch.zeros(B, T, C)\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        x_expression = torch.mean(x[b, :t+1], 0)\n",
        "        xbow[b, t] = x_expression\n",
        "\n",
        "print(xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuKWNTK8gIf_",
        "outputId": "9790643f-fe43-4926-b567-b3ea5d2c99fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 再用上面那个tril的矩阵乘法来表示\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "print('wei_tril:')\n",
        "print(wei)\n",
        "\n",
        "\n",
        "# 这里keepdim=True shape是(8, 1)所以除法就是每行除以对应的那一行的1个sum出来的数\n",
        "# keepdim=False, shape是(8, ), 那就是每一行从左到右每个元素都找到对应的去除\n",
        "wei_sum = wei.sum(1, keepdim=True)\n",
        "\n",
        "print('wei_sum:')\n",
        "print(wei_sum)\n",
        "wei = wei / wei_sum\n",
        "\n",
        "\n",
        "print('wei:')\n",
        "print(wei)\n",
        "\n",
        "xbow2 = wei @ x\n",
        "\n",
        "# 发现太小的值，这里[1,5,1] 是 0.0020，会导致allclose判断为False，\n",
        "# 这个问题很有意思，我记得21年跑这个代码没有这个问题，现在出现了。我也运行了一下Andrej的原始notebook也是Fasle\n",
        "# 说明大概率是python和torch的版本升级导致的，可能是seed rand的数字不一样了\n",
        "print('xbow2:')\n",
        "print(xbow2[1][5][1])\n",
        "print(xbow2[1][5][1] - xbow[1][5][1])\n",
        "torch.allclose(xbow[1][5][1], xbow2[1][5][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K30_1hzjUhb",
        "outputId": "6f172fa1-3415-4486-90ec-7d3e588e0ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei_tril:\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei_sum:\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.],\n",
            "        [5.],\n",
            "        [6.],\n",
            "        [7.],\n",
            "        [8.]])\n",
            "wei:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "xbow2:\n",
            "tensor(0.0020)\n",
            "tensor(-3.2363e-08)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 用softmax\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = torch.where(wei == 0.0, float('-inf'), wei)\n",
        "\n",
        "\"\"\"\n",
        "在PyTorch中，`F.softmax` 函数用于沿着指定的维度对输入张量进行softmax操作。对于张量 `wei`，其形状为 `(T, T)`，让我们分析一下 `F.softmax(wei, 1)` 和 `F.softmax(wei, -1)` 的含义：\n",
        "\n",
        "1. **`F.softmax(wei, 1)`**:\n",
        "   - 这里的 `1` 表示在第1个维度（即列方向）上进行softmax操作。对于每一行，`F.softmax` 会计算该行内的softmax值。\n",
        "\n",
        "2. **`F.softmax(wei, -1)`**:\n",
        "   - 这里的 `-1` 表示在最后一个维度上进行softmax操作。对于二维张量 `(T, T)` 来说，最后一个维度也是列方向（即维度1）。因此，`F.softmax(wei, -1)` 实际上与 `F.softmax(wei, 1)` 是等价的。\n",
        "\n",
        "总结来说，`F.softmax(wei, 1)` 和 `F.softmax(wei, -1)` 在形状为 `(T, T)` 的张量上执行时，是相同的操作，因为它们都在列方向上计算softmax。\n",
        "\"\"\"\n",
        "wei = F.softmax(wei, 1)\n",
        "print('wei:')\n",
        "print(wei)\n",
        "\n",
        "xbow3 = wei @ x\n",
        "\n",
        "# xbow2和xbow3是一样的\n",
        "# 因为xbow2和xbow3都是先除再加，xbow的mean是先加再除\n",
        "torch.allclose(xbow2, xbow3)"
      ],
      "metadata": {
        "id": "55UHB8xEqJEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35995ec7-8985-4a67-ac73-23c2cef4e327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 终于来了！自注意力self-attention\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 2, 16, 8\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "header_size = 4\n",
        "\n",
        "key = nn.Linear(C, header_size)\n",
        "query = nn.Linear(C, header_size)\n",
        "value = nn.Linear(C, header_size)\n",
        "\n",
        "# k.shape (B, T, head_size)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "v = value(x)\n",
        "\n",
        "# wei.shape 应该是(B, T, T)\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(wei == 0.0, float('-inf')) - 1\n",
        "# 使用 unsqueeze 在第0维度增加一个维度，然后使用 repeat 复制 B 份\n",
        "wei = wei.unsqueeze(0).repeat(B, 1, 1)\n",
        "wei += q @ k.transpose(1, 2) # 这里之前写成了v,写错了！！！！！！ q应该和k来算wei\n",
        "\n",
        "wei = F.softmax(wei, 2)\n",
        "\n",
        "# x_attented.shape (B, T, head_size)\n",
        "x_attented = wei @ v\n",
        "\n",
        "x_attented.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVdYLk_4eDFv",
        "outputId": "e762dd56-550c-4cb0-cbda-4fdf69a57789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 16, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3BIWez3s2klu"
      ],
      "authorship_tag": "ABX9TyPqZ6LRtA3u8LKjtkpjlR1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}